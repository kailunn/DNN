{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python373jvsc74a57bd0b63bd473a8c232b2ace8ef860f703af4c41e312cdf6bd38b38171e63b2f29f41",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "eration at 40, loss: 0.44189\n",
      "Iteration at 50, loss: 0.44065\n",
      "epo 11 , acc: 16.680392156862744 % training time: 10.927873611450195\n",
      "Iteration at 10, loss: 0.44268\n",
      "Iteration at 20, loss: 0.44156\n",
      "Iteration at 30, loss: 0.43710\n",
      "Iteration at 40, loss: 0.44083\n",
      "Iteration at 50, loss: 0.43721\n",
      "epo 12 , acc: 16.515686274509804 % training time: 10.361630201339722\n",
      "Iteration at 10, loss: 0.43717\n",
      "Iteration at 20, loss: 0.43788\n",
      "Iteration at 30, loss: 0.43620\n",
      "Iteration at 40, loss: 0.43597\n",
      "Iteration at 50, loss: 0.43607\n",
      "epo 13 , acc: 16.590196078431372 % training time: 10.997636795043945\n",
      "Iteration at 10, loss: 0.43362\n",
      "Iteration at 20, loss: 0.43198\n",
      "Iteration at 30, loss: 0.43334\n",
      "Iteration at 40, loss: 0.43055\n",
      "Iteration at 50, loss: 0.43458\n",
      "epo 14 , acc: 16.856862745098038 % training time: 10.745713472366333\n",
      "Iteration at 10, loss: 0.43860\n",
      "Iteration at 20, loss: 0.43284\n",
      "Iteration at 30, loss: 0.43455\n",
      "Iteration at 40, loss: 0.43244\n",
      "Iteration at 50, loss: 0.43383\n",
      "epo 15 , acc: 16.70392156862745 % training time: 11.031424522399902\n",
      "Iteration at 10, loss: 0.43152\n",
      "Iteration at 20, loss: 0.43385\n",
      "Iteration at 30, loss: 0.43195\n",
      "Iteration at 40, loss: 0.43093\n",
      "Iteration at 50, loss: 0.42968\n",
      "epo 16 , acc: 16.65490196078431 % training time: 10.34246039390564\n",
      "Iteration at 10, loss: 0.43358\n",
      "Iteration at 20, loss: 0.42759\n",
      "Iteration at 30, loss: 0.43057\n",
      "Iteration at 40, loss: 0.42953\n",
      "Iteration at 50, loss: 0.42642\n",
      "epo 17 , acc: 16.74313725490196 % training time: 10.633845806121826\n",
      "Iteration at 10, loss: 0.42626\n",
      "Iteration at 20, loss: 0.43090\n",
      "Iteration at 30, loss: 0.42502\n",
      "Iteration at 40, loss: 0.42583\n",
      "Iteration at 50, loss: 0.42487\n",
      "epo 18 , acc: 16.431372549019606 % training time: 10.346331596374512\n",
      "Iteration at 10, loss: 0.42719\n",
      "Iteration at 20, loss: 0.42440\n",
      "Iteration at 30, loss: 0.42551\n",
      "Iteration at 40, loss: 0.43137\n",
      "Iteration at 50, loss: 0.42673\n",
      "epo 19 , acc: 16.71764705882353 % training time: 10.38126254081726\n",
      "Iteration at 10, loss: 0.42531\n",
      "Iteration at 20, loss: 0.42539\n",
      "Iteration at 30, loss: 0.42305\n",
      "Iteration at 40, loss: 0.42637\n",
      "Iteration at 50, loss: 0.42189\n",
      "epo 20 , acc: 16.564705882352943 % training time: 10.413626194000244\n",
      "Iteration at 10, loss: 0.42321\n",
      "Iteration at 20, loss: 0.42149\n",
      "Iteration at 30, loss: 0.42597\n",
      "Iteration at 40, loss: 0.42451\n",
      "Iteration at 50, loss: 0.42263\n",
      "epo 21 , acc: 16.519607843137255 % training time: 10.583631753921509\n",
      "Iteration at 10, loss: 0.42223\n",
      "Iteration at 20, loss: 0.42151\n",
      "Iteration at 30, loss: 0.42039\n",
      "Iteration at 40, loss: 0.42422\n",
      "Iteration at 50, loss: 0.42286\n",
      "epo 22 , acc: 16.819607843137256 % training time: 10.799664974212646\n",
      "Iteration at 10, loss: 0.41843\n",
      "Iteration at 20, loss: 0.42040\n",
      "Iteration at 30, loss: 0.42033\n",
      "Iteration at 40, loss: 0.41378\n",
      "Iteration at 50, loss: 0.41868\n",
      "epo 23 , acc: 16.637254901960784 % training time: 10.505123853683472\n",
      "Iteration at 10, loss: 0.41812\n",
      "Iteration at 20, loss: 0.41354\n",
      "Iteration at 30, loss: 0.42184\n",
      "Iteration at 40, loss: 0.41752\n",
      "Iteration at 50, loss: 0.41301\n",
      "epo 24 , acc: 16.70392156862745 % training time: 11.102867364883423\n",
      "Iteration at 10, loss: 0.41757\n",
      "Iteration at 20, loss: 0.41507\n",
      "Iteration at 30, loss: 0.41507\n",
      "Iteration at 40, loss: 0.41699\n",
      "Iteration at 50, loss: 0.41477\n",
      "epo 25 , acc: 16.531372549019608 % training time: 10.33049464225769\n",
      "Iteration at 10, loss: 0.41228\n",
      "Iteration at 20, loss: 0.41482\n",
      "Iteration at 30, loss: 0.41369\n",
      "Iteration at 40, loss: 0.41196\n",
      "Iteration at 50, loss: 0.41085\n",
      "epo 26 , acc: 16.217647058823527 % training time: 10.923156499862671\n",
      "Iteration at 10, loss: 0.41530\n",
      "Iteration at 20, loss: 0.41131\n",
      "Iteration at 30, loss: 0.41245\n",
      "Iteration at 40, loss: 0.41173\n",
      "Iteration at 50, loss: 0.41166\n",
      "epo 27 , acc: 16.715686274509803 % training time: 10.52065110206604\n",
      "Iteration at 10, loss: 0.41288\n",
      "Iteration at 20, loss: 0.41295\n",
      "Iteration at 30, loss: 0.41008\n",
      "Iteration at 40, loss: 0.40740\n",
      "Iteration at 50, loss: 0.40326\n",
      "epo 28 , acc: 16.96470588235294 % training time: 11.161481857299805\n",
      "Iteration at 10, loss: 0.40643\n",
      "Iteration at 20, loss: 0.40995\n",
      "Iteration at 30, loss: 0.40748\n",
      "Iteration at 40, loss: 0.40425\n",
      "Iteration at 50, loss: 0.40706\n",
      "epo 29 , acc: 16.61764705882353 % training time: 10.340306043624878\n",
      "Iteration at 10, loss: 0.40528\n",
      "Iteration at 20, loss: 0.40726\n",
      "Iteration at 30, loss: 0.40540\n",
      "Iteration at 40, loss: 0.40443\n",
      "Iteration at 50, loss: 0.39949\n",
      "epo 30 , acc: 16.645098039215686 % training time: 10.899998903274536\n",
      "Iteration at 10, loss: 0.40319\n",
      "Iteration at 20, loss: 0.40307\n",
      "Iteration at 30, loss: 0.40486\n",
      "Iteration at 40, loss: 0.40123\n",
      "Iteration at 50, loss: 0.40360\n",
      "epo 31 , acc: 16.566666666666666 % training time: 10.61728048324585\n",
      "Iteration at 10, loss: 0.40368\n",
      "Iteration at 20, loss: 0.40246\n",
      "Iteration at 30, loss: 0.40184\n",
      "Iteration at 40, loss: 0.39817\n",
      "Iteration at 50, loss: 0.40047\n",
      "epo 32 , acc: 16.37450980392157 % training time: 10.726589679718018\n",
      "Iteration at 10, loss: 0.39734\n",
      "Iteration at 20, loss: 0.39705\n",
      "Iteration at 30, loss: 0.40615\n",
      "Iteration at 40, loss: 0.39950\n",
      "Iteration at 50, loss: 0.39844\n",
      "epo 33 , acc: 16.839215686274507 % training time: 10.455786228179932\n",
      "Iteration at 10, loss: 0.40049\n",
      "Iteration at 20, loss: 0.40294\n",
      "Iteration at 30, loss: 0.39588\n",
      "Iteration at 40, loss: 0.39720\n",
      "Iteration at 50, loss: 0.39747\n",
      "epo 34 , acc: 16.731372549019607 % training time: 10.844741582870483\n",
      "Iteration at 10, loss: 0.39737\n",
      "Iteration at 20, loss: 0.39900\n",
      "Iteration at 30, loss: 0.39681\n",
      "Iteration at 40, loss: 0.39381\n",
      "Iteration at 50, loss: 0.39796\n",
      "epo 35 , acc: 16.652941176470588 % training time: 10.717000722885132\n",
      "Iteration at 10, loss: 0.39468\n",
      "Iteration at 20, loss: 0.39735\n",
      "Iteration at 30, loss: 0.39525\n",
      "Iteration at 40, loss: 0.39742\n",
      "Iteration at 50, loss: 0.39348\n",
      "epo 36 , acc: 16.433333333333334 % training time: 10.357506275177002\n",
      "Iteration at 10, loss: 0.39543\n",
      "Iteration at 20, loss: 0.39828\n",
      "Iteration at 30, loss: 0.39606\n",
      "Iteration at 40, loss: 0.39371\n",
      "Iteration at 50, loss: 0.39213\n",
      "epo 37 , acc: 16.394117647058824 % training time: 10.853121042251587\n",
      "Iteration at 10, loss: 0.39396\n",
      "Iteration at 20, loss: 0.39294\n",
      "Iteration at 30, loss: 0.39266\n",
      "Iteration at 40, loss: 0.39477\n",
      "Iteration at 50, loss: 0.39237\n",
      "epo 38 , acc: 16.558823529411764 % training time: 10.749030351638794\n",
      "Iteration at 10, loss: 0.39398\n",
      "Iteration at 20, loss: 0.38836\n",
      "Iteration at 30, loss: 0.39283\n",
      "Iteration at 40, loss: 0.39128\n",
      "Iteration at 50, loss: 0.39453\n",
      "epo 39 , acc: 17.031372549019608 % training time: 10.817172288894653\n",
      "Iteration at 10, loss: 0.39125\n",
      "Iteration at 20, loss: 0.39028\n",
      "Iteration at 30, loss: 0.38668\n",
      "Iteration at 40, loss: 0.38985\n",
      "Iteration at 50, loss: 0.38919\n",
      "epo 40 , acc: 16.507843137254902 % training time: 10.775549173355103\n",
      "Iteration at 10, loss: 0.38627\n",
      "Iteration at 20, loss: 0.38881\n",
      "Iteration at 30, loss: 0.39260\n",
      "Iteration at 40, loss: 0.39039\n",
      "Iteration at 50, loss: 0.39013\n",
      "epo 41 , acc: 16.53333333333333 % training time: 10.397917985916138\n",
      "Iteration at 10, loss: 0.39270\n",
      "Iteration at 20, loss: 0.38785\n",
      "Iteration at 30, loss: 0.39108\n",
      "Iteration at 40, loss: 0.39052\n",
      "Iteration at 50, loss: 0.38632\n",
      "epo 42 , acc: 16.852941176470587 % training time: 10.82448935508728\n",
      "Iteration at 10, loss: 0.38731\n",
      "Iteration at 20, loss: 0.38607\n",
      "Iteration at 30, loss: 0.38982\n",
      "Iteration at 40, loss: 0.39032\n",
      "Iteration at 50, loss: 0.38595\n",
      "epo 43 , acc: 16.51372549019608 % training time: 10.41554594039917\n",
      "Iteration at 10, loss: 0.38761\n",
      "Iteration at 20, loss: 0.38691\n",
      "Iteration at 30, loss: 0.39015\n",
      "Iteration at 40, loss: 0.38735\n",
      "Iteration at 50, loss: 0.38525\n",
      "epo 44 , acc: 16.750980392156865 % training time: 10.344832420349121\n",
      "Iteration at 10, loss: 0.38541\n",
      "Iteration at 20, loss: 0.38682\n",
      "Iteration at 30, loss: 0.38738\n",
      "Iteration at 40, loss: 0.39204\n",
      "Iteration at 50, loss: 0.38764\n",
      "epo 45 , acc: 16.75686274509804 % training time: 10.367858171463013\n",
      "Iteration at 10, loss: 0.38453\n",
      "Iteration at 20, loss: 0.38946\n",
      "Iteration at 30, loss: 0.38406\n",
      "Iteration at 40, loss: 0.38600\n",
      "Iteration at 50, loss: 0.38538\n",
      "epo 46 , acc: 16.492156862745098 % training time: 10.717310190200806\n",
      "Iteration at 10, loss: 0.38505\n",
      "Iteration at 20, loss: 0.38859\n",
      "Iteration at 30, loss: 0.38269\n",
      "Iteration at 40, loss: 0.38704\n",
      "Iteration at 50, loss: 0.38499\n",
      "epo 47 , acc: 16.837254901960787 % training time: 10.862905502319336\n",
      "Iteration at 10, loss: 0.38561\n",
      "Iteration at 20, loss: 0.38454\n",
      "Iteration at 30, loss: 0.38702\n",
      "Iteration at 40, loss: 0.38539\n",
      "Iteration at 50, loss: 0.38100\n",
      "epo 48 , acc: 16.735294117647058 % training time: 10.394606828689575\n",
      "Iteration at 10, loss: 0.38552\n",
      "Iteration at 20, loss: 0.38502\n",
      "Iteration at 30, loss: 0.38236\n",
      "Iteration at 40, loss: 0.38245\n",
      "Iteration at 50, loss: 0.38090\n",
      "epo 49 , acc: 16.71764705882353 % training time: 10.46991491317749\n",
      "Iteration at 10, loss: 0.38124\n",
      "Iteration at 20, loss: 0.38042\n",
      "Iteration at 30, loss: 0.38556\n",
      "Iteration at 40, loss: 0.38609\n",
      "Iteration at 50, loss: 0.38805\n",
      "epo 50 , acc: 16.723529411764705 % training time: 10.340057134628296\n",
      "Iteration at 10, loss: 0.38421\n",
      "Iteration at 20, loss: 0.38681\n",
      "Iteration at 30, loss: 0.37913\n",
      "Iteration at 40, loss: 0.37988\n",
      "Iteration at 50, loss: 0.38059\n",
      "epo 51 , acc: 16.733333333333334 % training time: 10.946866989135742\n",
      "Iteration at 10, loss: 0.38219\n",
      "Iteration at 20, loss: 0.37899\n",
      "Iteration at 30, loss: 0.38136\n",
      "Iteration at 40, loss: 0.38176\n",
      "Iteration at 50, loss: 0.38317\n",
      "epo 52 , acc: 17.058823529411764 % training time: 10.741706371307373\n",
      "Iteration at 10, loss: 0.38151\n",
      "Iteration at 20, loss: 0.37789\n",
      "Iteration at 30, loss: 0.38263\n",
      "Iteration at 40, loss: 0.38025\n",
      "Iteration at 50, loss: 0.38292\n",
      "epo 53 , acc: 16.472549019607843 % training time: 10.700954914093018\n",
      "Iteration at 10, loss: 0.38111\n",
      "Iteration at 20, loss: 0.37913\n",
      "Iteration at 30, loss: 0.37964\n",
      "Iteration at 40, loss: 0.38287\n",
      "Iteration at 50, loss: 0.38126\n",
      "epo 54 , acc: 16.754901960784313 % training time: 10.41212797164917\n",
      "Iteration at 10, loss: 0.38250\n",
      "Iteration at 20, loss: 0.37608\n",
      "Iteration at 30, loss: 0.38224\n",
      "Iteration at 40, loss: 0.37955\n",
      "Iteration at 50, loss: 0.37935\n",
      "epo 55 , acc: 16.46470588235294 % training time: 10.848159074783325\n",
      "Iteration at 10, loss: 0.37749\n",
      "Iteration at 20, loss: 0.37795\n",
      "Iteration at 30, loss: 0.37443\n",
      "Iteration at 40, loss: 0.38362\n",
      "Iteration at 50, loss: 0.38521\n",
      "epo 56 , acc: 16.839215686274507 % training time: 10.772701501846313\n",
      "Iteration at 10, loss: 0.38205\n",
      "Iteration at 20, loss: 0.37836\n",
      "Iteration at 30, loss: 0.38248\n",
      "Iteration at 40, loss: 0.37877\n",
      "Iteration at 50, loss: 0.37730\n",
      "epo 57 , acc: 16.70980392156863 % training time: 10.860136985778809\n",
      "Iteration at 10, loss: 0.37745\n",
      "Iteration at 20, loss: 0.37922\n",
      "Iteration at 30, loss: 0.38059\n",
      "Iteration at 40, loss: 0.37798\n",
      "Iteration at 50, loss: 0.37863\n",
      "epo 58 , acc: 16.7921568627451 % training time: 10.760424137115479\n",
      "Iteration at 10, loss: 0.37940\n",
      "Iteration at 20, loss: 0.37975\n",
      "Iteration at 30, loss: 0.37638\n",
      "Iteration at 40, loss: 0.38078\n",
      "Iteration at 50, loss: 0.37732\n",
      "epo 59 , acc: 16.776470588235295 % training time: 10.777074575424194\n",
      "Iteration at 10, loss: 0.37444\n",
      "Iteration at 20, loss: 0.38034\n",
      "Iteration at 30, loss: 0.37975\n",
      "Iteration at 40, loss: 0.38001\n",
      "Iteration at 50, loss: 0.37763\n",
      "epo 60 , acc: 16.71764705882353 % training time: 11.1730477809906\n",
      "Iteration at 10, loss: 0.37617\n",
      "Iteration at 20, loss: 0.37937\n",
      "Iteration at 30, loss: 0.37853\n",
      "Iteration at 40, loss: 0.37753\n",
      "Iteration at 50, loss: 0.37673\n",
      "epo 61 , acc: 16.70980392156863 % training time: 10.952120065689087\n",
      "Iteration at 10, loss: 0.37653\n",
      "Iteration at 20, loss: 0.38005\n",
      "Iteration at 30, loss: 0.37522\n",
      "Iteration at 40, loss: 0.37836\n",
      "Iteration at 50, loss: 0.37571\n",
      "epo 62 , acc: 16.83529411764706 % training time: 10.292427062988281\n",
      "Iteration at 10, loss: 0.37583\n",
      "Iteration at 20, loss: 0.37708\n",
      "Iteration at 30, loss: 0.37717\n",
      "Iteration at 40, loss: 0.37711\n",
      "Iteration at 50, loss: 0.37567\n",
      "epo 63 , acc: 16.570588235294117 % training time: 11.681201934814453\n",
      "Iteration at 10, loss: 0.37723\n",
      "Iteration at 20, loss: 0.38003\n",
      "Iteration at 30, loss: 0.37945\n",
      "Iteration at 40, loss: 0.37716\n",
      "Iteration at 50, loss: 0.37292\n",
      "epo 64 , acc: 16.72156862745098 % training time: 11.178153038024902\n",
      "Iteration at 10, loss: 0.37721\n",
      "Iteration at 20, loss: 0.37765\n",
      "Iteration at 30, loss: 0.37270\n",
      "Iteration at 40, loss: 0.37816\n",
      "Iteration at 50, loss: 0.37692\n",
      "epo 65 , acc: 16.711764705882352 % training time: 10.857051849365234\n",
      "Iteration at 10, loss: 0.37365\n",
      "Iteration at 20, loss: 0.37510\n",
      "Iteration at 30, loss: 0.37392\n",
      "Iteration at 40, loss: 0.37780\n",
      "Iteration at 50, loss: 0.37815\n",
      "epo 66 , acc: 16.65490196078431 % training time: 10.539666891098022\n",
      "Iteration at 10, loss: 0.37119\n",
      "Iteration at 20, loss: 0.37266\n",
      "Iteration at 30, loss: 0.37630\n",
      "Iteration at 40, loss: 0.37824\n",
      "Iteration at 50, loss: 0.37681\n",
      "epo 67 , acc: 16.733333333333334 % training time: 10.57806944847107\n",
      "Iteration at 10, loss: 0.37763\n",
      "Iteration at 20, loss: 0.38029\n",
      "Iteration at 30, loss: 0.37596\n",
      "Iteration at 40, loss: 0.37299\n",
      "Iteration at 50, loss: 0.37883\n",
      "epo 68 , acc: 16.552941176470586 % training time: 10.738808155059814\n",
      "Iteration at 10, loss: 0.37308\n",
      "Iteration at 20, loss: 0.37502\n",
      "Iteration at 30, loss: 0.37421\n",
      "Iteration at 40, loss: 0.37627\n",
      "Iteration at 50, loss: 0.37377\n",
      "epo 69 , acc: 16.64313725490196 % training time: 10.470941543579102\n",
      "Iteration at 10, loss: 0.37029\n",
      "Iteration at 20, loss: 0.37366\n",
      "Iteration at 30, loss: 0.37301\n",
      "Iteration at 40, loss: 0.37579\n",
      "Iteration at 50, loss: 0.37522\n",
      "epo 70 , acc: 17.005882352941175 % training time: 11.086548805236816\n",
      "Iteration at 10, loss: 0.37104\n",
      "Iteration at 20, loss: 0.37729\n",
      "Iteration at 30, loss: 0.37543\n",
      "Iteration at 40, loss: 0.37652\n",
      "Iteration at 50, loss: 0.37457\n",
      "epo 71 , acc: 16.63529411764706 % training time: 10.492875099182129\n",
      "Iteration at 10, loss: 0.36742\n",
      "Iteration at 20, loss: 0.37377\n",
      "Iteration at 30, loss: 0.37128\n",
      "Iteration at 40, loss: 0.37442\n",
      "Iteration at 50, loss: 0.37436\n",
      "epo 72 , acc: 16.40392156862745 % training time: 10.82863712310791\n",
      "Iteration at 10, loss: 0.37522\n",
      "Iteration at 20, loss: 0.37820\n",
      "Iteration at 30, loss: 0.37852\n",
      "Iteration at 40, loss: 0.37468\n",
      "Iteration at 50, loss: 0.37576\n",
      "epo 73 , acc: 16.727450980392156 % training time: 11.368502855300903\n",
      "Iteration at 10, loss: 0.37551\n",
      "Iteration at 20, loss: 0.37296\n",
      "Iteration at 30, loss: 0.37526\n",
      "Iteration at 40, loss: 0.37418\n",
      "Iteration at 50, loss: 0.37320\n",
      "epo 74 , acc: 16.69019607843137 % training time: 10.721288681030273\n",
      "Iteration at 10, loss: 0.37270\n",
      "Iteration at 20, loss: 0.37262\n",
      "Iteration at 30, loss: 0.37020\n",
      "Iteration at 40, loss: 0.37584\n",
      "Iteration at 50, loss: 0.37900\n",
      "epo 75 , acc: 16.566666666666666 % training time: 11.082072973251343\n",
      "Iteration at 10, loss: 0.37485\n",
      "Iteration at 20, loss: 0.37235\n",
      "Iteration at 30, loss: 0.37056\n",
      "Iteration at 40, loss: 0.37207\n",
      "Iteration at 50, loss: 0.37910\n",
      "epo 76 , acc: 16.731372549019607 % training time: 10.805638551712036\n",
      "Iteration at 10, loss: 0.37588\n",
      "Iteration at 20, loss: 0.37322\n",
      "Iteration at 30, loss: 0.37467\n",
      "Iteration at 40, loss: 0.37581\n",
      "Iteration at 50, loss: 0.37406\n",
      "epo 77 , acc: 16.87843137254902 % training time: 10.877180576324463\n",
      "Iteration at 10, loss: 0.37459\n",
      "Iteration at 20, loss: 0.37139\n",
      "Iteration at 30, loss: 0.37312\n",
      "Iteration at 40, loss: 0.37379\n",
      "Iteration at 50, loss: 0.37586\n",
      "epo 78 , acc: 16.392156862745097 % training time: 10.656615495681763\n",
      "Iteration at 10, loss: 0.37030\n",
      "Iteration at 20, loss: 0.37701\n",
      "Iteration at 30, loss: 0.37377\n",
      "Iteration at 40, loss: 0.37429\n",
      "Iteration at 50, loss: 0.37537\n",
      "epo 79 , acc: 16.7 % training time: 11.150706052780151\n",
      "Iteration at 10, loss: 0.37603\n",
      "Iteration at 20, loss: 0.37366\n",
      "Iteration at 30, loss: 0.37252\n",
      "Iteration at 40, loss: 0.37453\n",
      "Iteration at 50, loss: 0.37142\n",
      "epo 80 , acc: 16.550980392156863 % training time: 11.243764638900757\n",
      "Iteration at 10, loss: 0.37184\n",
      "Iteration at 20, loss: 0.36969\n",
      "Iteration at 30, loss: 0.37598\n",
      "Iteration at 40, loss: 0.37445\n",
      "Iteration at 50, loss: 0.37215\n",
      "epo 81 , acc: 16.7078431372549 % training time: 10.608951807022095\n",
      "Iteration at 10, loss: 0.37108\n",
      "Iteration at 20, loss: 0.37049\n",
      "Iteration at 30, loss: 0.36932\n",
      "Iteration at 40, loss: 0.37409\n",
      "Iteration at 50, loss: 0.36826\n",
      "epo 82 , acc: 16.856862745098038 % training time: 10.887836456298828\n",
      "Iteration at 10, loss: 0.36998\n",
      "Iteration at 20, loss: 0.37078\n",
      "Iteration at 30, loss: 0.37239\n",
      "Iteration at 40, loss: 0.37146\n",
      "Iteration at 50, loss: 0.36735\n",
      "epo 83 , acc: 16.698039215686276 % training time: 11.020912170410156\n",
      "Iteration at 10, loss: 0.37025\n",
      "Iteration at 20, loss: 0.37373\n",
      "Iteration at 30, loss: 0.36755\n",
      "Iteration at 40, loss: 0.37134\n",
      "Iteration at 50, loss: 0.36997\n",
      "epo 84 , acc: 16.837254901960787 % training time: 10.473753452301025\n",
      "Iteration at 10, loss: 0.37052\n",
      "Iteration at 20, loss: 0.36770\n",
      "Iteration at 30, loss: 0.36564\n",
      "Iteration at 40, loss: 0.37047\n",
      "Iteration at 50, loss: 0.37376\n",
      "epo 85 , acc: 16.841176470588234 % training time: 10.655866622924805\n",
      "Iteration at 10, loss: 0.37072\n",
      "Iteration at 20, loss: 0.37159\n",
      "Iteration at 30, loss: 0.37571\n",
      "Iteration at 40, loss: 0.37276\n",
      "Iteration at 50, loss: 0.37354\n",
      "epo 86 , acc: 16.69607843137255 % training time: 10.652232885360718\n",
      "Iteration at 10, loss: 0.37139\n",
      "Iteration at 20, loss: 0.37315\n",
      "Iteration at 30, loss: 0.37514\n",
      "Iteration at 40, loss: 0.36608\n",
      "Iteration at 50, loss: 0.36484\n",
      "epo 87 , acc: 16.337254901960783 % training time: 10.580223798751831\n",
      "Iteration at 10, loss: 0.37107\n",
      "Iteration at 20, loss: 0.36836\n",
      "Iteration at 30, loss: 0.36993\n",
      "Iteration at 40, loss: 0.36925\n",
      "Iteration at 50, loss: 0.36682\n",
      "epo 88 , acc: 16.645098039215686 % training time: 10.439031839370728\n",
      "Iteration at 10, loss: 0.36715\n",
      "Iteration at 20, loss: 0.37081\n",
      "Iteration at 30, loss: 0.37087\n",
      "Iteration at 40, loss: 0.37091\n",
      "Iteration at 50, loss: 0.37109\n",
      "epo 89 , acc: 16.6156862745098 % training time: 10.722154378890991\n",
      "Iteration at 10, loss: 0.37098\n",
      "Iteration at 20, loss: 0.37100\n",
      "Iteration at 30, loss: 0.36795\n",
      "Iteration at 40, loss: 0.36699\n",
      "Iteration at 50, loss: 0.37138\n",
      "epo 90 , acc: 16.945098039215686 % training time: 10.670873165130615\n",
      "Iteration at 10, loss: 0.36639\n",
      "Iteration at 20, loss: 0.36979\n",
      "Iteration at 30, loss: 0.36905\n",
      "Iteration at 40, loss: 0.37140\n",
      "Iteration at 50, loss: 0.36687\n",
      "epo 91 , acc: 16.77843137254902 % training time: 12.195770263671875\n",
      "Iteration at 10, loss: 0.36963\n",
      "Iteration at 20, loss: 0.36740\n",
      "Iteration at 30, loss: 0.37077\n",
      "Iteration at 40, loss: 0.37283\n",
      "Iteration at 50, loss: 0.36930\n",
      "epo 92 , acc: 16.56078431372549 % training time: 13.110067367553711\n",
      "Iteration at 10, loss: 0.36487\n",
      "Iteration at 20, loss: 0.36425\n",
      "Iteration at 30, loss: 0.37226\n",
      "Iteration at 40, loss: 0.37060\n",
      "Iteration at 50, loss: 0.36864\n",
      "epo 93 , acc: 16.445098039215686 % training time: 10.87298321723938\n",
      "Iteration at 10, loss: 0.37056\n",
      "Iteration at 20, loss: 0.37163\n",
      "Iteration at 30, loss: 0.37004\n",
      "Iteration at 40, loss: 0.36855\n",
      "Iteration at 50, loss: 0.37178\n",
      "epo 94 , acc: 16.627450980392155 % training time: 14.275792837142944\n",
      "Iteration at 10, loss: 0.37007\n",
      "Iteration at 20, loss: 0.36712\n",
      "Iteration at 30, loss: 0.37308\n",
      "Iteration at 40, loss: 0.36842\n",
      "Iteration at 50, loss: 0.36774\n",
      "epo 95 , acc: 16.676470588235297 % training time: 13.69864797592163\n",
      "Iteration at 10, loss: 0.36469\n",
      "Iteration at 20, loss: 0.36991\n",
      "Iteration at 30, loss: 0.36856\n",
      "Iteration at 40, loss: 0.36908\n",
      "Iteration at 50, loss: 0.36478\n",
      "epo 96 , acc: 16.74313725490196 % training time: 14.350096702575684\n",
      "Iteration at 10, loss: 0.36638\n",
      "Iteration at 20, loss: 0.36814\n",
      "Iteration at 30, loss: 0.36631\n",
      "Iteration at 40, loss: 0.36935\n",
      "Iteration at 50, loss: 0.36374\n",
      "epo 97 , acc: 16.472549019607843 % training time: 14.743338823318481\n",
      "Iteration at 10, loss: 0.36515\n",
      "Iteration at 20, loss: 0.36882\n",
      "Iteration at 30, loss: 0.37028\n",
      "Iteration at 40, loss: 0.37069\n",
      "Iteration at 50, loss: 0.36154\n",
      "epo 98 , acc: 16.70392156862745 % training time: 14.355498552322388\n",
      "Iteration at 10, loss: 0.37156\n",
      "Iteration at 20, loss: 0.36550\n",
      "Iteration at 30, loss: 0.36879\n",
      "Iteration at 40, loss: 0.36505\n",
      "Iteration at 50, loss: 0.37099\n",
      "epo 99 , acc: 16.66078431372549 % training time: 14.001777410507202\n",
      "Iteration at 10, loss: 0.36933\n",
      "Iteration at 20, loss: 0.37253\n",
      "Iteration at 30, loss: 0.36472\n",
      "Iteration at 40, loss: 0.36761\n",
      "Iteration at 50, loss: 0.36786\n",
      "epo 100 , acc: 16.71764705882353 % training time: 13.510611295700073\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Apr  6 22:35:27 2021\n",
    "\n",
    "1-1a-trainnig :\n",
    "\n",
    "此神經網路架構為input layer ==> hidden layer ==> output layer的幾單結構。三層的節點數量分別為1024, 100, 6。 訓練的batch size一開始設先為1000，最後匯出Loss及Accuracy, Loss曲線有明顯向下收斂，但Accuracy仍然不穩定，可能與網路結構較簡單及神經元數量較少有關。 Epoch=100。Lr=0.05\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train = np.load(r'C:\\Users\\Jimmy\\Desktop\\suzy\\DL homework\\DL_HW1\\Problem1\\MedMNIST\\train.npz')\n",
    "test = np.load(r'C:\\Users\\Jimmy\\Desktop\\suzy\\DL homework\\DL_HW1\\Problem1\\MedMNIST\\test.npz')\n",
    "#list(X_train.keys()) #image,label\n",
    "#np.shape(X_test['image'])\n",
    "X_train = np.array(train['image'])/255\n",
    "Y_train = np.array(train['label'])\n",
    "\n",
    "Y_train = Y_train.reshape(51000,1)\n",
    "yy=Y_train.reshape(51000,)\n",
    "Y_train_=pd.get_dummies(yy)\n",
    "\n",
    "X_train = X_train.reshape(51000, 32*32).astype('float32')\n",
    "\n",
    "\n",
    "x = X_train\n",
    "y = Y_train_\n",
    "\n",
    "\n",
    "# activation function\n",
    "  \n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "# weight initializing => random\n",
    "def initial_weight(a, b):   #layer input & output size\n",
    "    l =[]\n",
    "    for i in range(a * b):\n",
    "        l.append(np.random.randn())\n",
    "    return np.array(l).reshape(a, b)\n",
    "  \n",
    "def f_forward(x, w1, w2):\n",
    "    # hidden\n",
    "    z1 = x.dot(w1)# input from layer 1 \n",
    "    a1 = sigmoid(z1)# out put of layer 2 \n",
    "      \n",
    "    # Output layer\n",
    "    z2 = a1.dot(w2)# input of out layer\n",
    "    a2 = sigmoid(z2)# output of out layer\n",
    "    return a2\n",
    "   \n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    \n",
    "    for i in range(exp_x.shape[0]):\n",
    "        sum_val = exp_x[i].sum()\n",
    "        for j in range(exp_x.shape[1]):\n",
    "            exp_x[i,j] = exp_x[i,j]/sum_val\n",
    "        \n",
    "    return exp_x\n",
    "\n",
    "\n",
    "# cross entropy\n",
    "def loss(y,t): #y is predicted result; t is target\n",
    "    #x=np.log(softmax(x))\n",
    "    y = softmax(y)\n",
    "    \n",
    "    Cross_en_loss = abs(np.sum(-t*np.log(y+1e-6)))/1000\n",
    "    \n",
    "    return Cross_en_loss/1000\n",
    "\n",
    "\n",
    "    \n",
    "# Back propagation  \n",
    "def back_prop(x, y, w1, w2, alpha,batch_size):\n",
    "     \n",
    "    w1_adj = 0\n",
    "    w2_adj = 0\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        x_input = x[i]\n",
    "        y_input = y[i]\n",
    "        z1 = x_input.dot(w1) #(10,)\n",
    "        a1 = sigmoid(z1)  #(10,)1\n",
    "          \n",
    "        # Output layer\n",
    "        z2 = a1.dot(w2)#(6,)\n",
    "        a2 = sigmoid(z2)# output of out layer (6,)\n",
    "        \n",
    "        d2 =(a2-y_input) #(6,)\n",
    "        \n",
    "        # Gradient for w1 and w2\n",
    "        #w1_adj = x.reshape(len(x[1]),1).dot(d1.reshape(1,len(d1))) \n",
    "        w2_adj += np.reshape(d2*sigmoid(z2)*(1-sigmoid(z2)),(-1,1)).dot(a1.reshape(1,-1)).T  #(10,6)\n",
    "        \n",
    "        w1_adj += np.reshape((d2*sigmoid(z2)*(1-sigmoid(z2))).T.dot(w2.T)*sigmoid(z1)*(1-sigmoid(z1)),(-1,1)).dot(np.reshape(x_input,(1,-1))).T   #(1024,10)\n",
    "        \n",
    "        \n",
    "    w1_adj = w1_adj/batch_size \n",
    "    w2_adj = w2_adj/batch_size \n",
    "    \n",
    "    # Updating parameters\n",
    "    w1 = w1-(alpha*(w1_adj))\n",
    "    w2 = w2-(alpha*(w2_adj))\n",
    "      \n",
    "    return w1, w2\n",
    "  \n",
    "def train(x, Y, y_true, w1, w2, alpha = 0.10, epoch = 10, batch_size=1000):\n",
    "    acc =[]\n",
    "    losss =[]\n",
    "    \n",
    "    for j in range(epoch):\n",
    "        l =[]\n",
    "        pred = []\n",
    "        pred = np.array(pred)\n",
    "        \n",
    "        idx=np.arange(51000)\n",
    "        np.random.shuffle(idx)\n",
    "        x = x[idx,:]\n",
    "        Y = Y[idx,:]\n",
    "        yyy = y_true[idx]\n",
    "        start = time.time()\n",
    "        for i in range(0,51):\n",
    "                       \n",
    "            x_train= x[(i*batch_size):(i+1)*batch_size,:]\n",
    "            target = Y[(i*batch_size):(i+1)*batch_size,:]\n",
    "            Y_train = f_forward(x_train,w1,w2)\n",
    "            yhat = predict(x_train,w1,w2)\n",
    "            \n",
    "            loss_value = loss(Y_train,target)\n",
    "            \n",
    "            pred = np.append(pred,yhat)\n",
    "            l.append(loss_value)\n",
    "            \n",
    "            w1, w2 = back_prop(x_train, target, w1, w2, alpha, batch_size)\n",
    "                \n",
    "            if(i%10==0 and i!=0):\n",
    "                print('Iteration at %d, loss: %.5f' %(i, l[i]))\n",
    "                \n",
    "        final = time.time()\n",
    "        training_time = final-start\n",
    "        print(\"epo\", j + 1, \", acc:\", sum(yyy==pred)/len(yyy)*100,\"% training time:\", training_time)   \n",
    "        #acc.append((1-(sum(l)/len(x)))*100)\n",
    "        acc.append(sum(yyy==pred)/len(yyy)*100)\n",
    "        losss.append(sum(l)/len(x))\n",
    "    return acc, losss, w1, w2\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "def predict(x, w1, w2):\n",
    "    y_pred = f_forward(x, w1, w2)\n",
    "    \n",
    "    maxm = 0\n",
    "    array = []\n",
    "    for i in range(0,y_pred.shape[0]):\n",
    "        target = 0\n",
    "        maxm=y_pred[i,0]\n",
    "        for j in range(0,y_pred.shape[1]):\n",
    "            \n",
    "            if(j==0):\n",
    "                next\n",
    "            \n",
    "            if(maxm<=y_pred[i,j]):\n",
    "                maxm = y_pred[i,j]\n",
    "                target = j\n",
    "            elif(maxm>y_pred[i,j]):\n",
    "                next\n",
    "                \n",
    "        array.append(target)\n",
    "    \n",
    "    return array\n",
    "\n",
    "w1 = initial_weight(1024, 50)\n",
    "w2 = initial_weight(50, 6)\n",
    "\n",
    "y = np.array(y)\n",
    "\n",
    "acc, losss, w1, w2 = train(x, y, yy, w1, w2, 0.05, 100, batch_size=1000)\n",
    "y_pred= predict(X_train,w1,w2)\n",
    "#y_pred = f_forward(X_test, w1, w2)\n",
    "plt.plot(np.array(losss))\n",
    "plt.ylabel('LOSS')\n",
    "plt.show()\n",
    "plt.plot(np.array(acc))\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0, loss: 0.02915\n",
      "Iteration at 80, loss: 0.02971\n",
      "Iteration at 90, loss: 0.02992\n",
      "epo 14 , acc: 18.657279356298716 % training time: 2.0180752277374268\n",
      "Iteration at 10, loss: 0.02893\n",
      "Iteration at 20, loss: 0.02974\n",
      "Iteration at 30, loss: 0.02898\n",
      "Iteration at 40, loss: 0.02916\n",
      "Iteration at 50, loss: 0.02930\n",
      "Iteration at 60, loss: 0.02882\n",
      "Iteration at 70, loss: 0.03005\n",
      "Iteration at 80, loss: 0.02866\n",
      "Iteration at 90, loss: 0.02947\n",
      "epo 15 , acc: 17.60120693990445 % training time: 1.9331319332122803\n",
      "Iteration at 10, loss: 0.02879\n",
      "Iteration at 20, loss: 0.02931\n",
      "Iteration at 30, loss: 0.02923\n",
      "Iteration at 40, loss: 0.02964\n",
      "Iteration at 50, loss: 0.02980\n",
      "Iteration at 60, loss: 0.02950\n",
      "Iteration at 70, loss: 0.02970\n",
      "Iteration at 80, loss: 0.03124\n",
      "Iteration at 90, loss: 0.03160\n",
      "epo 16 , acc: 19.13502640181041 % training time: 1.9657719135284424\n",
      "Iteration at 10, loss: 0.02921\n",
      "Iteration at 20, loss: 0.03005\n",
      "Iteration at 30, loss: 0.02915\n",
      "Iteration at 40, loss: 0.02930\n",
      "Iteration at 50, loss: 0.02868\n",
      "Iteration at 60, loss: 0.02870\n",
      "Iteration at 70, loss: 0.02948\n",
      "Iteration at 80, loss: 0.02895\n",
      "Iteration at 90, loss: 0.02855\n",
      "epo 17 , acc: 18.8207191350264 % training time: 2.048862934112549\n",
      "Iteration at 10, loss: 0.02924\n",
      "Iteration at 20, loss: 0.02941\n",
      "Iteration at 30, loss: 0.02894\n",
      "Iteration at 40, loss: 0.02924\n",
      "Iteration at 50, loss: 0.02890\n",
      "Iteration at 60, loss: 0.02826\n",
      "Iteration at 70, loss: 0.02903\n",
      "Iteration at 80, loss: 0.02797\n",
      "Iteration at 90, loss: 0.02949\n",
      "epo 18 , acc: 18.581845612270556 % training time: 2.1643974781036377\n",
      "Iteration at 10, loss: 0.02983\n",
      "Iteration at 20, loss: 0.02967\n",
      "Iteration at 30, loss: 0.03035\n",
      "Iteration at 40, loss: 0.02876\n",
      "Iteration at 50, loss: 0.03087\n",
      "Iteration at 60, loss: 0.03001\n",
      "Iteration at 70, loss: 0.02834\n",
      "Iteration at 80, loss: 0.02859\n",
      "Iteration at 90, loss: 0.02918\n",
      "epo 19 , acc: 18.016092532059343 % training time: 2.2675065994262695\n",
      "Iteration at 10, loss: 0.02866\n",
      "Iteration at 20, loss: 0.03001\n",
      "Iteration at 30, loss: 0.02920\n",
      "Iteration at 40, loss: 0.02977\n",
      "Iteration at 50, loss: 0.02883\n",
      "Iteration at 60, loss: 0.02939\n",
      "Iteration at 70, loss: 0.02937\n",
      "Iteration at 80, loss: 0.02811\n",
      "Iteration at 90, loss: 0.02992\n",
      "epo 20 , acc: 19.386472215237617 % training time: 2.176579475402832\n",
      "Iteration at 10, loss: 0.02831\n",
      "Iteration at 20, loss: 0.02952\n",
      "Iteration at 30, loss: 0.02885\n",
      "Iteration at 40, loss: 0.02822\n",
      "Iteration at 50, loss: 0.02857\n",
      "Iteration at 60, loss: 0.02929\n",
      "Iteration at 70, loss: 0.02835\n",
      "Iteration at 80, loss: 0.03005\n",
      "Iteration at 90, loss: 0.02967\n",
      "epo 21 , acc: 18.46869499622831 % training time: 2.176426649093628\n",
      "Iteration at 10, loss: 0.02892\n",
      "Iteration at 20, loss: 0.02889\n",
      "Iteration at 30, loss: 0.02956\n",
      "Iteration at 40, loss: 0.02891\n",
      "Iteration at 50, loss: 0.02975\n",
      "Iteration at 60, loss: 0.02952\n",
      "Iteration at 70, loss: 0.02859\n",
      "Iteration at 80, loss: 0.02829\n",
      "Iteration at 90, loss: 0.02944\n",
      "epo 22 , acc: 17.840080462660296 % training time: 2.1523306369781494\n",
      "Iteration at 10, loss: 0.02836\n",
      "Iteration at 20, loss: 0.02780\n",
      "Iteration at 30, loss: 0.02952\n",
      "Iteration at 40, loss: 0.02907\n",
      "Iteration at 50, loss: 0.02943\n",
      "Iteration at 60, loss: 0.02858\n",
      "Iteration at 70, loss: 0.02935\n",
      "Iteration at 80, loss: 0.02918\n",
      "Iteration at 90, loss: 0.02914\n",
      "epo 23 , acc: 18.116670857430222 % training time: 2.1582863330841064\n",
      "Iteration at 10, loss: 0.02916\n",
      "Iteration at 20, loss: 0.02886\n",
      "Iteration at 30, loss: 0.02914\n",
      "Iteration at 40, loss: 0.02880\n",
      "Iteration at 50, loss: 0.02871\n",
      "Iteration at 60, loss: 0.02856\n",
      "Iteration at 70, loss: 0.02813\n",
      "Iteration at 80, loss: 0.02968\n",
      "Iteration at 90, loss: 0.02841\n",
      "epo 24 , acc: 17.94065878803118 % training time: 2.113649606704712\n",
      "Iteration at 10, loss: 0.02878\n",
      "Iteration at 20, loss: 0.02928\n",
      "Iteration at 30, loss: 0.02875\n",
      "Iteration at 40, loss: 0.02983\n",
      "Iteration at 50, loss: 0.02656\n",
      "Iteration at 60, loss: 0.02959\n",
      "Iteration at 70, loss: 0.02867\n",
      "Iteration at 80, loss: 0.02856\n",
      "Iteration at 90, loss: 0.02824\n",
      "epo 25 , acc: 17.63892381191853 % training time: 2.0397086143493652\n",
      "Iteration at 10, loss: 0.02806\n",
      "Iteration at 20, loss: 0.02807\n",
      "Iteration at 30, loss: 0.02826\n",
      "Iteration at 40, loss: 0.03046\n",
      "Iteration at 50, loss: 0.02919\n",
      "Iteration at 60, loss: 0.02914\n",
      "Iteration at 70, loss: 0.02805\n",
      "Iteration at 80, loss: 0.02939\n",
      "Iteration at 90, loss: 0.02900\n",
      "epo 26 , acc: 17.1737490570782 % training time: 2.3416550159454346\n",
      "Iteration at 10, loss: 0.02791\n",
      "Iteration at 20, loss: 0.02880\n",
      "Iteration at 30, loss: 0.02926\n",
      "Iteration at 40, loss: 0.02945\n",
      "Iteration at 50, loss: 0.02689\n",
      "Iteration at 60, loss: 0.03020\n",
      "Iteration at 70, loss: 0.02866\n",
      "Iteration at 80, loss: 0.02876\n",
      "Iteration at 90, loss: 0.02754\n",
      "epo 27 , acc: 18.405833542871513 % training time: 2.1001553535461426\n",
      "Iteration at 10, loss: 0.02814\n",
      "Iteration at 20, loss: 0.02817\n",
      "Iteration at 30, loss: 0.02882\n",
      "Iteration at 40, loss: 0.02804\n",
      "Iteration at 50, loss: 0.02909\n",
      "Iteration at 60, loss: 0.02852\n",
      "Iteration at 70, loss: 0.02738\n",
      "Iteration at 80, loss: 0.02992\n",
      "Iteration at 90, loss: 0.02833\n",
      "epo 28 , acc: 18.242393764143827 % training time: 2.159291982650757\n",
      "Iteration at 10, loss: 0.02818\n",
      "Iteration at 20, loss: 0.02870\n",
      "Iteration at 30, loss: 0.02782\n",
      "Iteration at 40, loss: 0.02762\n",
      "Iteration at 50, loss: 0.02927\n",
      "Iteration at 60, loss: 0.02749\n",
      "Iteration at 70, loss: 0.02790\n",
      "Iteration at 80, loss: 0.02846\n",
      "Iteration at 90, loss: 0.02912\n",
      "epo 29 , acc: 17.852652753331657 % training time: 2.04020357131958\n",
      "Iteration at 10, loss: 0.02934\n",
      "Iteration at 20, loss: 0.02873\n",
      "Iteration at 30, loss: 0.02848\n",
      "Iteration at 40, loss: 0.02835\n",
      "Iteration at 50, loss: 0.02775\n",
      "Iteration at 60, loss: 0.02944\n",
      "Iteration at 70, loss: 0.02734\n",
      "Iteration at 80, loss: 0.02754\n",
      "Iteration at 90, loss: 0.02918\n",
      "epo 30 , acc: 18.104098566758864 % training time: 2.0027501583099365\n",
      "Iteration at 10, loss: 0.02820\n",
      "Iteration at 20, loss: 0.02876\n",
      "Iteration at 30, loss: 0.02833\n",
      "Iteration at 40, loss: 0.03001\n",
      "Iteration at 50, loss: 0.02959\n",
      "Iteration at 60, loss: 0.02837\n",
      "Iteration at 70, loss: 0.02809\n",
      "Iteration at 80, loss: 0.02936\n",
      "Iteration at 90, loss: 0.02895\n",
      "epo 31 , acc: 18.104098566758864 % training time: 1.9899940490722656\n",
      "Iteration at 10, loss: 0.02789\n",
      "Iteration at 20, loss: 0.02875\n",
      "Iteration at 30, loss: 0.02809\n",
      "Iteration at 40, loss: 0.02802\n",
      "Iteration at 50, loss: 0.02893\n",
      "Iteration at 60, loss: 0.02807\n",
      "Iteration at 70, loss: 0.02886\n",
      "Iteration at 80, loss: 0.02924\n",
      "Iteration at 90, loss: 0.02788\n",
      "epo 32 , acc: 17.877797334674376 % training time: 1.9858760833740234\n",
      "Iteration at 10, loss: 0.02853\n",
      "Iteration at 20, loss: 0.02763\n",
      "Iteration at 30, loss: 0.02963\n",
      "Iteration at 40, loss: 0.02804\n",
      "Iteration at 50, loss: 0.02898\n",
      "Iteration at 60, loss: 0.02989\n",
      "Iteration at 70, loss: 0.02829\n",
      "Iteration at 80, loss: 0.02944\n",
      "Iteration at 90, loss: 0.02882\n",
      "epo 33 , acc: 17.67664068393261 % training time: 1.9748477935791016\n",
      "Iteration at 10, loss: 0.02737\n",
      "Iteration at 20, loss: 0.02845\n",
      "Iteration at 30, loss: 0.02748\n",
      "Iteration at 40, loss: 0.02839\n",
      "Iteration at 50, loss: 0.02877\n",
      "Iteration at 60, loss: 0.02830\n",
      "Iteration at 70, loss: 0.02892\n",
      "Iteration at 80, loss: 0.02822\n",
      "Iteration at 90, loss: 0.02875\n",
      "epo 34 , acc: 18.254966054815185 % training time: 2.180286169052124\n",
      "Iteration at 10, loss: 0.02890\n",
      "Iteration at 20, loss: 0.02945\n",
      "Iteration at 30, loss: 0.02804\n",
      "Iteration at 40, loss: 0.02750\n",
      "Iteration at 50, loss: 0.02865\n",
      "Iteration at 60, loss: 0.02853\n",
      "Iteration at 70, loss: 0.02813\n",
      "Iteration at 80, loss: 0.02835\n",
      "Iteration at 90, loss: 0.02828\n",
      "epo 35 , acc: 18.317827508171987 % training time: 2.4532368183135986\n",
      "Iteration at 10, loss: 0.02708\n",
      "Iteration at 20, loss: 0.02851\n",
      "Iteration at 30, loss: 0.02848\n",
      "Iteration at 40, loss: 0.02859\n",
      "Iteration at 50, loss: 0.02787\n",
      "Iteration at 60, loss: 0.02904\n",
      "Iteration at 70, loss: 0.02812\n",
      "Iteration at 80, loss: 0.02772\n",
      "Iteration at 90, loss: 0.02726\n",
      "epo 36 , acc: 17.877797334674376 % training time: 2.2161247730255127\n",
      "Iteration at 10, loss: 0.02789\n",
      "Iteration at 20, loss: 0.02863\n",
      "Iteration at 30, loss: 0.02859\n",
      "Iteration at 40, loss: 0.02746\n",
      "Iteration at 50, loss: 0.02780\n",
      "Iteration at 60, loss: 0.03030\n",
      "Iteration at 70, loss: 0.02923\n",
      "Iteration at 80, loss: 0.02915\n",
      "Iteration at 90, loss: 0.02781\n",
      "epo 37 , acc: 17.425194870505408 % training time: 2.0791821479797363\n",
      "Iteration at 10, loss: 0.02889\n",
      "Iteration at 20, loss: 0.02762\n",
      "Iteration at 30, loss: 0.02763\n",
      "Iteration at 40, loss: 0.02757\n",
      "Iteration at 50, loss: 0.02758\n",
      "Iteration at 60, loss: 0.02864\n",
      "Iteration at 70, loss: 0.02796\n",
      "Iteration at 80, loss: 0.02734\n",
      "Iteration at 90, loss: 0.02933\n",
      "epo 38 , acc: 18.141815438772944 % training time: 2.154834270477295\n",
      "Iteration at 10, loss: 0.02821\n",
      "Iteration at 20, loss: 0.02841\n",
      "Iteration at 30, loss: 0.02786\n",
      "Iteration at 40, loss: 0.02808\n",
      "Iteration at 50, loss: 0.02757\n",
      "Iteration at 60, loss: 0.02825\n",
      "Iteration at 70, loss: 0.02893\n",
      "Iteration at 80, loss: 0.02861\n",
      "Iteration at 90, loss: 0.02787\n",
      "epo 39 , acc: 18.04123711340206 % training time: 2.131730079650879\n",
      "Iteration at 10, loss: 0.02713\n",
      "Iteration at 20, loss: 0.02921\n",
      "Iteration at 30, loss: 0.02873\n",
      "Iteration at 40, loss: 0.02657\n",
      "Iteration at 50, loss: 0.02929\n",
      "Iteration at 60, loss: 0.02771\n",
      "Iteration at 70, loss: 0.02769\n",
      "Iteration at 80, loss: 0.02789\n",
      "Iteration at 90, loss: 0.02794\n",
      "epo 40 , acc: 18.280110636157907 % training time: 2.2719132900238037\n",
      "Iteration at 10, loss: 0.02732\n",
      "Iteration at 20, loss: 0.02812\n",
      "Iteration at 30, loss: 0.02872\n",
      "Iteration at 40, loss: 0.02903\n",
      "Iteration at 50, loss: 0.02664\n",
      "Iteration at 60, loss: 0.02937\n",
      "Iteration at 70, loss: 0.02651\n",
      "Iteration at 80, loss: 0.02804\n",
      "Iteration at 90, loss: 0.02805\n",
      "epo 41 , acc: 18.116670857430222 % training time: 2.035659074783325\n",
      "Iteration at 10, loss: 0.02734\n",
      "Iteration at 20, loss: 0.02981\n",
      "Iteration at 30, loss: 0.02878\n",
      "Iteration at 40, loss: 0.02876\n",
      "Iteration at 50, loss: 0.02815\n",
      "Iteration at 60, loss: 0.02847\n",
      "Iteration at 70, loss: 0.02799\n",
      "Iteration at 80, loss: 0.02666\n",
      "Iteration at 90, loss: 0.02762\n",
      "epo 42 , acc: 18.091526276087503 % training time: 2.067077159881592\n",
      "Iteration at 10, loss: 0.02785\n",
      "Iteration at 20, loss: 0.02922\n",
      "Iteration at 30, loss: 0.02736\n",
      "Iteration at 40, loss: 0.02811\n",
      "Iteration at 50, loss: 0.02784\n",
      "Iteration at 60, loss: 0.02818\n",
      "Iteration at 70, loss: 0.02774\n",
      "Iteration at 80, loss: 0.02913\n",
      "Iteration at 90, loss: 0.02839\n",
      "epo 43 , acc: 18.74528539099824 % training time: 2.2585091590881348\n",
      "Iteration at 10, loss: 0.02757\n",
      "Iteration at 20, loss: 0.02637\n",
      "Iteration at 30, loss: 0.02902\n",
      "Iteration at 40, loss: 0.02816\n",
      "Iteration at 50, loss: 0.02739\n",
      "Iteration at 60, loss: 0.02734\n",
      "Iteration at 70, loss: 0.02796\n",
      "Iteration at 80, loss: 0.02895\n",
      "Iteration at 90, loss: 0.02843\n",
      "epo 44 , acc: 18.74528539099824 % training time: 2.193864107131958\n",
      "Iteration at 10, loss: 0.02873\n",
      "Iteration at 20, loss: 0.02822\n",
      "Iteration at 30, loss: 0.02895\n",
      "Iteration at 40, loss: 0.02689\n",
      "Iteration at 50, loss: 0.02698\n",
      "Iteration at 60, loss: 0.02657\n",
      "Iteration at 70, loss: 0.02686\n",
      "Iteration at 80, loss: 0.02809\n",
      "Iteration at 90, loss: 0.02770\n",
      "epo 45 , acc: 18.61956248428464 % training time: 2.252807378768921\n",
      "Iteration at 10, loss: 0.02862\n",
      "Iteration at 20, loss: 0.02787\n",
      "Iteration at 30, loss: 0.02658\n",
      "Iteration at 40, loss: 0.02780\n",
      "Iteration at 50, loss: 0.02791\n",
      "Iteration at 60, loss: 0.02717\n",
      "Iteration at 70, loss: 0.02713\n",
      "Iteration at 80, loss: 0.02807\n",
      "Iteration at 90, loss: 0.02703\n",
      "epo 46 , acc: 17.450339451848127 % training time: 2.114532709121704\n",
      "Iteration at 10, loss: 0.02753\n",
      "Iteration at 20, loss: 0.02684\n",
      "Iteration at 30, loss: 0.02676\n",
      "Iteration at 40, loss: 0.02751\n",
      "Iteration at 50, loss: 0.02842\n",
      "Iteration at 60, loss: 0.02770\n",
      "Iteration at 70, loss: 0.02550\n",
      "Iteration at 80, loss: 0.02701\n",
      "Iteration at 90, loss: 0.02821\n",
      "epo 47 , acc: 17.67664068393261 % training time: 2.1575427055358887\n",
      "Iteration at 10, loss: 0.02796\n",
      "Iteration at 20, loss: 0.02807\n",
      "Iteration at 30, loss: 0.02834\n",
      "Iteration at 40, loss: 0.02723\n",
      "Iteration at 50, loss: 0.02775\n",
      "Iteration at 60, loss: 0.02800\n",
      "Iteration at 70, loss: 0.02741\n",
      "Iteration at 80, loss: 0.02674\n",
      "Iteration at 90, loss: 0.02654\n",
      "epo 48 , acc: 17.58863464923309 % training time: 2.210022449493408\n",
      "Iteration at 10, loss: 0.02760\n",
      "Iteration at 20, loss: 0.02743\n",
      "Iteration at 30, loss: 0.02810\n",
      "Iteration at 40, loss: 0.02771\n",
      "Iteration at 50, loss: 0.02784\n",
      "Iteration at 60, loss: 0.02769\n",
      "Iteration at 70, loss: 0.02844\n",
      "Iteration at 80, loss: 0.02799\n",
      "Iteration at 90, loss: 0.02914\n",
      "epo 49 , acc: 18.33039979884335 % training time: 2.2323734760284424\n",
      "Iteration at 10, loss: 0.02854\n",
      "Iteration at 20, loss: 0.02773\n",
      "Iteration at 30, loss: 0.02754\n",
      "Iteration at 40, loss: 0.02694\n",
      "Iteration at 50, loss: 0.02771\n",
      "Iteration at 60, loss: 0.02903\n",
      "Iteration at 70, loss: 0.02740\n",
      "Iteration at 80, loss: 0.02807\n",
      "Iteration at 90, loss: 0.02909\n",
      "epo 50 , acc: 18.896152879054565 % training time: 2.1042566299438477\n",
      "Iteration at 10, loss: 0.02755\n",
      "Iteration at 20, loss: 0.02713\n",
      "Iteration at 30, loss: 0.02746\n",
      "Iteration at 40, loss: 0.02700\n",
      "Iteration at 50, loss: 0.02817\n",
      "Iteration at 60, loss: 0.02758\n",
      "Iteration at 70, loss: 0.02897\n",
      "Iteration at 80, loss: 0.02761\n",
      "Iteration at 90, loss: 0.02723\n",
      "epo 51 , acc: 17.9658033693739 % training time: 2.121673107147217\n",
      "Iteration at 10, loss: 0.02928\n",
      "Iteration at 20, loss: 0.02658\n",
      "Iteration at 30, loss: 0.02678\n",
      "Iteration at 40, loss: 0.02730\n",
      "Iteration at 50, loss: 0.02702\n",
      "Iteration at 60, loss: 0.02742\n",
      "Iteration at 70, loss: 0.02699\n",
      "Iteration at 80, loss: 0.02837\n",
      "Iteration at 90, loss: 0.02811\n",
      "epo 52 , acc: 18.078953985416142 % training time: 2.088186264038086\n",
      "Iteration at 10, loss: 0.02878\n",
      "Iteration at 20, loss: 0.02820\n",
      "Iteration at 30, loss: 0.02705\n",
      "Iteration at 40, loss: 0.02690\n",
      "Iteration at 50, loss: 0.02713\n",
      "Iteration at 60, loss: 0.02728\n",
      "Iteration at 70, loss: 0.02719\n",
      "Iteration at 80, loss: 0.02708\n",
      "Iteration at 90, loss: 0.02870\n",
      "epo 53 , acc: 18.06638169474478 % training time: 2.1257197856903076\n",
      "Iteration at 10, loss: 0.02796\n",
      "Iteration at 20, loss: 0.02804\n",
      "Iteration at 30, loss: 0.02835\n",
      "Iteration at 40, loss: 0.02861\n",
      "Iteration at 50, loss: 0.02700\n",
      "Iteration at 60, loss: 0.02908\n",
      "Iteration at 70, loss: 0.02724\n",
      "Iteration at 80, loss: 0.02727\n",
      "Iteration at 90, loss: 0.02742\n",
      "epo 54 , acc: 17.67664068393261 % training time: 2.1315479278564453\n",
      "Iteration at 10, loss: 0.02820\n",
      "Iteration at 20, loss: 0.02737\n",
      "Iteration at 30, loss: 0.02801\n",
      "Iteration at 40, loss: 0.02608\n",
      "Iteration at 50, loss: 0.02736\n",
      "Iteration at 60, loss: 0.02748\n",
      "Iteration at 70, loss: 0.02753\n",
      "Iteration at 80, loss: 0.02705\n",
      "Iteration at 90, loss: 0.02874\n",
      "epo 55 , acc: 17.488056323862207 % training time: 2.129695177078247\n",
      "Iteration at 10, loss: 0.02759\n",
      "Iteration at 20, loss: 0.02760\n",
      "Iteration at 30, loss: 0.02628\n",
      "Iteration at 40, loss: 0.02661\n",
      "Iteration at 50, loss: 0.02714\n",
      "Iteration at 60, loss: 0.02848\n",
      "Iteration at 70, loss: 0.02764\n",
      "Iteration at 80, loss: 0.02802\n",
      "Iteration at 90, loss: 0.02889\n",
      "epo 56 , acc: 18.581845612270556 % training time: 2.126573085784912\n",
      "Iteration at 10, loss: 0.02823\n",
      "Iteration at 20, loss: 0.02795\n",
      "Iteration at 30, loss: 0.02873\n",
      "Iteration at 40, loss: 0.02761\n",
      "Iteration at 50, loss: 0.02704\n",
      "Iteration at 60, loss: 0.02787\n",
      "Iteration at 70, loss: 0.02779\n",
      "Iteration at 80, loss: 0.02783\n",
      "Iteration at 90, loss: 0.02707\n",
      "epo 57 , acc: 17.14860447573548 % training time: 2.1229476928710938\n",
      "Iteration at 10, loss: 0.02859\n",
      "Iteration at 20, loss: 0.02687\n",
      "Iteration at 30, loss: 0.02771\n",
      "Iteration at 40, loss: 0.02737\n",
      "Iteration at 50, loss: 0.02821\n",
      "Iteration at 60, loss: 0.02683\n",
      "Iteration at 70, loss: 0.02721\n",
      "Iteration at 80, loss: 0.02727\n",
      "Iteration at 90, loss: 0.02797\n",
      "epo 58 , acc: 18.104098566758864 % training time: 2.093435287475586\n",
      "Iteration at 10, loss: 0.02774\n",
      "Iteration at 20, loss: 0.02691\n",
      "Iteration at 30, loss: 0.02702\n",
      "Iteration at 40, loss: 0.02712\n",
      "Iteration at 50, loss: 0.02702\n",
      "Iteration at 60, loss: 0.02708\n",
      "Iteration at 70, loss: 0.02832\n",
      "Iteration at 80, loss: 0.02845\n",
      "Iteration at 90, loss: 0.02823\n",
      "epo 59 , acc: 18.016092532059343 % training time: 2.0881876945495605\n",
      "Iteration at 10, loss: 0.02779\n",
      "Iteration at 20, loss: 0.02685\n",
      "Iteration at 30, loss: 0.02824\n",
      "Iteration at 40, loss: 0.02782\n",
      "Iteration at 50, loss: 0.02776\n",
      "Iteration at 60, loss: 0.02698\n",
      "Iteration at 70, loss: 0.02825\n",
      "Iteration at 80, loss: 0.02872\n",
      "Iteration at 90, loss: 0.02878\n",
      "epo 60 , acc: 17.802363590646216 % training time: 2.1236701011657715\n",
      "Iteration at 10, loss: 0.02720\n",
      "Iteration at 20, loss: 0.02749\n",
      "Iteration at 30, loss: 0.02718\n",
      "Iteration at 40, loss: 0.02847\n",
      "Iteration at 50, loss: 0.02678\n",
      "Iteration at 60, loss: 0.02789\n",
      "Iteration at 70, loss: 0.02747\n",
      "Iteration at 80, loss: 0.02740\n",
      "Iteration at 90, loss: 0.02770\n",
      "epo 61 , acc: 18.229821473472466 % training time: 2.112304210662842\n",
      "Iteration at 10, loss: 0.02745\n",
      "Iteration at 20, loss: 0.02835\n",
      "Iteration at 30, loss: 0.02793\n",
      "Iteration at 40, loss: 0.02698\n",
      "Iteration at 50, loss: 0.02796\n",
      "Iteration at 60, loss: 0.02761\n",
      "Iteration at 70, loss: 0.02742\n",
      "Iteration at 80, loss: 0.02569\n",
      "Iteration at 90, loss: 0.02737\n",
      "epo 62 , acc: 18.091526276087503 % training time: 2.1318466663360596\n",
      "Iteration at 10, loss: 0.02870\n",
      "Iteration at 20, loss: 0.02859\n",
      "Iteration at 30, loss: 0.02723\n",
      "Iteration at 40, loss: 0.02738\n",
      "Iteration at 50, loss: 0.02727\n",
      "Iteration at 60, loss: 0.02741\n",
      "Iteration at 70, loss: 0.02787\n",
      "Iteration at 80, loss: 0.02694\n",
      "Iteration at 90, loss: 0.02614\n",
      "epo 63 , acc: 19.09730952979633 % training time: 2.181849479675293\n",
      "Iteration at 10, loss: 0.02778\n",
      "Iteration at 20, loss: 0.02793\n",
      "Iteration at 30, loss: 0.02803\n",
      "Iteration at 40, loss: 0.02723\n",
      "Iteration at 50, loss: 0.02657\n",
      "Iteration at 60, loss: 0.02728\n",
      "Iteration at 70, loss: 0.02843\n",
      "Iteration at 80, loss: 0.02701\n",
      "Iteration at 90, loss: 0.02601\n",
      "epo 64 , acc: 17.689212974603976 % training time: 2.180832862854004\n",
      "Iteration at 10, loss: 0.02927\n",
      "Iteration at 20, loss: 0.02906\n",
      "Iteration at 30, loss: 0.02597\n",
      "Iteration at 40, loss: 0.02777\n",
      "Iteration at 50, loss: 0.02666\n",
      "Iteration at 60, loss: 0.02649\n",
      "Iteration at 70, loss: 0.02677\n",
      "Iteration at 80, loss: 0.02840\n",
      "Iteration at 90, loss: 0.02681\n",
      "epo 65 , acc: 17.689212974603976 % training time: 2.1609442234039307\n",
      "Iteration at 10, loss: 0.02587\n",
      "Iteration at 20, loss: 0.02693\n",
      "Iteration at 30, loss: 0.02590\n",
      "Iteration at 40, loss: 0.02728\n",
      "Iteration at 50, loss: 0.02699\n",
      "Iteration at 60, loss: 0.02662\n",
      "Iteration at 70, loss: 0.02920\n",
      "Iteration at 80, loss: 0.02807\n",
      "Iteration at 90, loss: 0.02642\n",
      "epo 66 , acc: 18.68242393764144 % training time: 2.323368549346924\n",
      "Iteration at 10, loss: 0.02733\n",
      "Iteration at 20, loss: 0.02701\n",
      "Iteration at 30, loss: 0.02662\n",
      "Iteration at 40, loss: 0.02746\n",
      "Iteration at 50, loss: 0.02716\n",
      "Iteration at 60, loss: 0.02767\n",
      "Iteration at 70, loss: 0.02716\n",
      "Iteration at 80, loss: 0.02624\n",
      "Iteration at 90, loss: 0.02861\n",
      "epo 67 , acc: 17.865225044003015 % training time: 2.2576935291290283\n",
      "Iteration at 10, loss: 0.02714\n",
      "Iteration at 20, loss: 0.02637\n",
      "Iteration at 30, loss: 0.02775\n",
      "Iteration at 40, loss: 0.02773\n",
      "Iteration at 50, loss: 0.02722\n",
      "Iteration at 60, loss: 0.02711\n",
      "Iteration at 70, loss: 0.02595\n",
      "Iteration at 80, loss: 0.02879\n",
      "Iteration at 90, loss: 0.02657\n",
      "epo 68 , acc: 17.97837566004526 % training time: 2.202263355255127\n",
      "Iteration at 10, loss: 0.02529\n",
      "Iteration at 20, loss: 0.02772\n",
      "Iteration at 30, loss: 0.02914\n",
      "Iteration at 40, loss: 0.02782\n",
      "Iteration at 50, loss: 0.02707\n",
      "Iteration at 60, loss: 0.02750\n",
      "Iteration at 70, loss: 0.02674\n",
      "Iteration at 80, loss: 0.02784\n",
      "Iteration at 90, loss: 0.02613\n",
      "epo 69 , acc: 17.450339451848127 % training time: 2.2604751586914062\n",
      "Iteration at 10, loss: 0.02693\n",
      "Iteration at 20, loss: 0.02828\n",
      "Iteration at 30, loss: 0.02776\n",
      "Iteration at 40, loss: 0.02666\n",
      "Iteration at 50, loss: 0.02679\n",
      "Iteration at 60, loss: 0.02726\n",
      "Iteration at 70, loss: 0.02666\n",
      "Iteration at 80, loss: 0.02738\n",
      "Iteration at 90, loss: 0.02659\n",
      "epo 70 , acc: 17.814935881317577 % training time: 2.2191896438598633\n",
      "Iteration at 10, loss: 0.02679\n",
      "Iteration at 20, loss: 0.02805\n",
      "Iteration at 30, loss: 0.02657\n",
      "Iteration at 40, loss: 0.02650\n",
      "Iteration at 50, loss: 0.02644\n",
      "Iteration at 60, loss: 0.02855\n",
      "Iteration at 70, loss: 0.02536\n",
      "Iteration at 80, loss: 0.02734\n",
      "Iteration at 90, loss: 0.02696\n",
      "epo 71 , acc: 18.078953985416142 % training time: 2.109795570373535\n",
      "Iteration at 10, loss: 0.02631\n",
      "Iteration at 20, loss: 0.02791\n",
      "Iteration at 30, loss: 0.02682\n",
      "Iteration at 40, loss: 0.02684\n",
      "Iteration at 50, loss: 0.02758\n",
      "Iteration at 60, loss: 0.02795\n",
      "Iteration at 70, loss: 0.02813\n",
      "Iteration at 80, loss: 0.02717\n",
      "Iteration at 90, loss: 0.02824\n",
      "epo 72 , acc: 18.003520241387978 % training time: 2.1233582496643066\n",
      "Iteration at 10, loss: 0.02821\n",
      "Iteration at 20, loss: 0.02689\n",
      "Iteration at 30, loss: 0.02661\n",
      "Iteration at 40, loss: 0.02694\n",
      "Iteration at 50, loss: 0.02763\n",
      "Iteration at 60, loss: 0.02653\n",
      "Iteration at 70, loss: 0.02718\n",
      "Iteration at 80, loss: 0.02839\n",
      "Iteration at 90, loss: 0.02646\n",
      "epo 73 , acc: 18.192104601458386 % training time: 2.0607075691223145\n",
      "Iteration at 10, loss: 0.02699\n",
      "Iteration at 20, loss: 0.02780\n",
      "Iteration at 30, loss: 0.02722\n",
      "Iteration at 40, loss: 0.02814\n",
      "Iteration at 50, loss: 0.02714\n",
      "Iteration at 60, loss: 0.02768\n",
      "Iteration at 70, loss: 0.02658\n",
      "Iteration at 80, loss: 0.02735\n",
      "Iteration at 90, loss: 0.02738\n",
      "epo 74 , acc: 17.261755091777722 % training time: 2.082655906677246\n",
      "Iteration at 10, loss: 0.02715\n",
      "Iteration at 20, loss: 0.02675\n",
      "Iteration at 30, loss: 0.02738\n",
      "Iteration at 40, loss: 0.02829\n",
      "Iteration at 50, loss: 0.02892\n",
      "Iteration at 60, loss: 0.02768\n",
      "Iteration at 70, loss: 0.02698\n",
      "Iteration at 80, loss: 0.02816\n",
      "Iteration at 90, loss: 0.02746\n",
      "epo 75 , acc: 18.45612270555695 % training time: 2.1124353408813477\n",
      "Iteration at 10, loss: 0.02660\n",
      "Iteration at 20, loss: 0.02748\n",
      "Iteration at 30, loss: 0.02733\n",
      "Iteration at 40, loss: 0.02687\n",
      "Iteration at 50, loss: 0.02732\n",
      "Iteration at 60, loss: 0.02778\n",
      "Iteration at 70, loss: 0.02753\n",
      "Iteration at 80, loss: 0.02653\n",
      "Iteration at 90, loss: 0.02713\n",
      "epo 76 , acc: 18.053809404073423 % training time: 2.1071603298187256\n",
      "Iteration at 10, loss: 0.02779\n",
      "Iteration at 20, loss: 0.02762\n",
      "Iteration at 30, loss: 0.02805\n",
      "Iteration at 40, loss: 0.02769\n",
      "Iteration at 50, loss: 0.02757\n",
      "Iteration at 60, loss: 0.02732\n",
      "Iteration at 70, loss: 0.02657\n",
      "Iteration at 80, loss: 0.02667\n",
      "Iteration at 90, loss: 0.02648\n",
      "epo 77 , acc: 17.626351521247173 % training time: 2.096696615219116\n",
      "Iteration at 10, loss: 0.02703\n",
      "Iteration at 20, loss: 0.02675\n",
      "Iteration at 30, loss: 0.02747\n",
      "Iteration at 40, loss: 0.02753\n",
      "Iteration at 50, loss: 0.02720\n",
      "Iteration at 60, loss: 0.02771\n",
      "Iteration at 70, loss: 0.02710\n",
      "Iteration at 80, loss: 0.02799\n",
      "Iteration at 90, loss: 0.02762\n",
      "epo 78 , acc: 17.714357555946695 % training time: 2.1130146980285645\n",
      "Iteration at 10, loss: 0.02768\n",
      "Iteration at 20, loss: 0.02691\n",
      "Iteration at 30, loss: 0.02734\n",
      "Iteration at 40, loss: 0.02810\n",
      "Iteration at 50, loss: 0.02745\n",
      "Iteration at 60, loss: 0.02779\n",
      "Iteration at 70, loss: 0.02660\n",
      "Iteration at 80, loss: 0.02805\n",
      "Iteration at 90, loss: 0.02797\n",
      "epo 79 , acc: 18.204676892129747 % training time: 2.095273494720459\n",
      "Iteration at 10, loss: 0.02723\n",
      "Iteration at 20, loss: 0.02711\n",
      "Iteration at 30, loss: 0.02798\n",
      "Iteration at 40, loss: 0.02717\n",
      "Iteration at 50, loss: 0.02898\n",
      "Iteration at 60, loss: 0.02703\n",
      "Iteration at 70, loss: 0.02751\n",
      "Iteration at 80, loss: 0.02733\n",
      "Iteration at 90, loss: 0.02596\n",
      "epo 80 , acc: 17.664068393261253 % training time: 2.075103521347046\n",
      "Iteration at 10, loss: 0.02828\n",
      "Iteration at 20, loss: 0.02641\n",
      "Iteration at 30, loss: 0.02645\n",
      "Iteration at 40, loss: 0.02725\n",
      "Iteration at 50, loss: 0.02676\n",
      "Iteration at 60, loss: 0.02799\n",
      "Iteration at 70, loss: 0.02733\n",
      "Iteration at 80, loss: 0.02783\n",
      "Iteration at 90, loss: 0.02751\n",
      "epo 81 , acc: 18.30525521750063 % training time: 2.0746142864227295\n",
      "Iteration at 10, loss: 0.02647\n",
      "Iteration at 20, loss: 0.02676\n",
      "Iteration at 30, loss: 0.02646\n",
      "Iteration at 40, loss: 0.02714\n",
      "Iteration at 50, loss: 0.02687\n",
      "Iteration at 60, loss: 0.02769\n",
      "Iteration at 70, loss: 0.02668\n",
      "Iteration at 80, loss: 0.02596\n",
      "Iteration at 90, loss: 0.02744\n",
      "epo 82 , acc: 17.95323107870254 % training time: 2.082928419113159\n",
      "Iteration at 10, loss: 0.02630\n",
      "Iteration at 20, loss: 0.02665\n",
      "Iteration at 30, loss: 0.02650\n",
      "Iteration at 40, loss: 0.02666\n",
      "Iteration at 50, loss: 0.02710\n",
      "Iteration at 60, loss: 0.02692\n",
      "Iteration at 70, loss: 0.02797\n",
      "Iteration at 80, loss: 0.02847\n",
      "Iteration at 90, loss: 0.02661\n",
      "epo 83 , acc: 17.701785265275333 % training time: 2.104300022125244\n",
      "Iteration at 10, loss: 0.02928\n",
      "Iteration at 20, loss: 0.02717\n",
      "Iteration at 30, loss: 0.02699\n",
      "Iteration at 40, loss: 0.02743\n",
      "Iteration at 50, loss: 0.02749\n",
      "Iteration at 60, loss: 0.02638\n",
      "Iteration at 70, loss: 0.02769\n",
      "Iteration at 80, loss: 0.02870\n",
      "Iteration at 90, loss: 0.02699\n",
      "epo 84 , acc: 17.739502137289413 % training time: 2.1233325004577637\n",
      "Iteration at 10, loss: 0.02733\n",
      "Iteration at 20, loss: 0.02640\n",
      "Iteration at 30, loss: 0.02690\n",
      "Iteration at 40, loss: 0.02617\n",
      "Iteration at 50, loss: 0.02856\n",
      "Iteration at 60, loss: 0.02786\n",
      "Iteration at 70, loss: 0.02695\n",
      "Iteration at 80, loss: 0.02759\n",
      "Iteration at 90, loss: 0.02769\n",
      "epo 85 , acc: 18.280110636157907 % training time: 2.102281332015991\n",
      "Iteration at 10, loss: 0.02731\n",
      "Iteration at 20, loss: 0.02713\n",
      "Iteration at 30, loss: 0.02785\n",
      "Iteration at 40, loss: 0.02694\n",
      "Iteration at 50, loss: 0.02647\n",
      "Iteration at 60, loss: 0.02812\n",
      "Iteration at 70, loss: 0.02708\n",
      "Iteration at 80, loss: 0.02767\n",
      "Iteration at 90, loss: 0.02804\n",
      "epo 86 , acc: 18.73271310032688 % training time: 2.0979928970336914\n",
      "Iteration at 10, loss: 0.02710\n",
      "Iteration at 20, loss: 0.02761\n",
      "Iteration at 30, loss: 0.02655\n",
      "Iteration at 40, loss: 0.02709\n",
      "Iteration at 50, loss: 0.02677\n",
      "Iteration at 60, loss: 0.02652\n",
      "Iteration at 70, loss: 0.02670\n",
      "Iteration at 80, loss: 0.02730\n",
      "Iteration at 90, loss: 0.02771\n",
      "epo 87 , acc: 18.016092532059343 % training time: 2.0869693756103516\n",
      "Iteration at 10, loss: 0.02781\n",
      "Iteration at 20, loss: 0.02840\n",
      "Iteration at 30, loss: 0.02652\n",
      "Iteration at 40, loss: 0.02817\n",
      "Iteration at 50, loss: 0.02708\n",
      "Iteration at 60, loss: 0.02702\n",
      "Iteration at 70, loss: 0.02693\n",
      "Iteration at 80, loss: 0.02688\n",
      "Iteration at 90, loss: 0.02829\n",
      "epo 88 , acc: 18.078953985416142 % training time: 2.0769565105438232\n",
      "Iteration at 10, loss: 0.02786\n",
      "Iteration at 20, loss: 0.02676\n",
      "Iteration at 30, loss: 0.02629\n",
      "Iteration at 40, loss: 0.02730\n",
      "Iteration at 50, loss: 0.02644\n",
      "Iteration at 60, loss: 0.02778\n",
      "Iteration at 70, loss: 0.02637\n",
      "Iteration at 80, loss: 0.02712\n",
      "Iteration at 90, loss: 0.02660\n",
      "epo 89 , acc: 17.91551420668846 % training time: 2.1324660778045654\n",
      "Iteration at 10, loss: 0.02642\n",
      "Iteration at 20, loss: 0.02800\n",
      "Iteration at 30, loss: 0.02638\n",
      "Iteration at 40, loss: 0.02615\n",
      "Iteration at 50, loss: 0.02629\n",
      "Iteration at 60, loss: 0.02668\n",
      "Iteration at 70, loss: 0.02681\n",
      "Iteration at 80, loss: 0.02777\n",
      "Iteration at 90, loss: 0.02784\n",
      "epo 90 , acc: 17.9658033693739 % training time: 2.155956983566284\n",
      "Iteration at 10, loss: 0.02704\n",
      "Iteration at 20, loss: 0.02678\n",
      "Iteration at 30, loss: 0.02746\n",
      "Iteration at 40, loss: 0.02718\n",
      "Iteration at 50, loss: 0.02669\n",
      "Iteration at 60, loss: 0.02877\n",
      "Iteration at 70, loss: 0.02592\n",
      "Iteration at 80, loss: 0.02666\n",
      "Iteration at 90, loss: 0.02899\n",
      "epo 91 , acc: 17.802363590646216 % training time: 2.115675449371338\n",
      "Iteration at 10, loss: 0.02635\n",
      "Iteration at 20, loss: 0.02768\n",
      "Iteration at 30, loss: 0.02599\n",
      "Iteration at 40, loss: 0.02777\n",
      "Iteration at 50, loss: 0.02625\n",
      "Iteration at 60, loss: 0.02670\n",
      "Iteration at 70, loss: 0.02678\n",
      "Iteration at 80, loss: 0.02700\n",
      "Iteration at 90, loss: 0.02725\n",
      "epo 92 , acc: 17.764646718632136 % training time: 2.0883121490478516\n",
      "Iteration at 10, loss: 0.02774\n",
      "Iteration at 20, loss: 0.02649\n",
      "Iteration at 30, loss: 0.02731\n",
      "Iteration at 40, loss: 0.02677\n",
      "Iteration at 50, loss: 0.02738\n",
      "Iteration at 60, loss: 0.02782\n",
      "Iteration at 70, loss: 0.02730\n",
      "Iteration at 80, loss: 0.02700\n",
      "Iteration at 90, loss: 0.02918\n",
      "epo 93 , acc: 17.500628614533568 % training time: 2.095684051513672\n",
      "Iteration at 10, loss: 0.02652\n",
      "Iteration at 20, loss: 0.02699\n",
      "Iteration at 30, loss: 0.02693\n",
      "Iteration at 40, loss: 0.02716\n",
      "Iteration at 50, loss: 0.02741\n",
      "Iteration at 60, loss: 0.02761\n",
      "Iteration at 70, loss: 0.02755\n",
      "Iteration at 80, loss: 0.02765\n",
      "Iteration at 90, loss: 0.02681\n",
      "epo 94 , acc: 17.299471963791802 % training time: 2.1137633323669434\n",
      "Iteration at 10, loss: 0.02693\n",
      "Iteration at 20, loss: 0.02737\n",
      "Iteration at 30, loss: 0.02700\n",
      "Iteration at 40, loss: 0.02733\n",
      "Iteration at 50, loss: 0.02705\n",
      "Iteration at 60, loss: 0.02717\n",
      "Iteration at 70, loss: 0.02703\n",
      "Iteration at 80, loss: 0.02703\n",
      "Iteration at 90, loss: 0.02755\n",
      "epo 95 , acc: 18.204676892129747 % training time: 2.070929765701294\n",
      "Iteration at 10, loss: 0.02642\n",
      "Iteration at 20, loss: 0.02604\n",
      "Iteration at 30, loss: 0.02796\n",
      "Iteration at 40, loss: 0.02824\n",
      "Iteration at 50, loss: 0.02748\n",
      "Iteration at 60, loss: 0.02678\n",
      "Iteration at 70, loss: 0.02752\n",
      "Iteration at 80, loss: 0.02767\n",
      "Iteration at 90, loss: 0.02594\n",
      "epo 96 , acc: 17.67664068393261 % training time: 2.153752326965332\n",
      "Iteration at 10, loss: 0.02711\n",
      "Iteration at 20, loss: 0.02653\n",
      "Iteration at 30, loss: 0.02783\n",
      "Iteration at 40, loss: 0.02625\n",
      "Iteration at 50, loss: 0.02739\n",
      "Iteration at 60, loss: 0.02678\n",
      "Iteration at 70, loss: 0.02754\n",
      "Iteration at 80, loss: 0.02676\n",
      "Iteration at 90, loss: 0.02611\n",
      "epo 97 , acc: 18.33039979884335 % training time: 2.0915682315826416\n",
      "Iteration at 10, loss: 0.02727\n",
      "Iteration at 20, loss: 0.02697\n",
      "Iteration at 30, loss: 0.02735\n",
      "Iteration at 40, loss: 0.02623\n",
      "Iteration at 50, loss: 0.02858\n",
      "Iteration at 60, loss: 0.02693\n",
      "Iteration at 70, loss: 0.02742\n",
      "Iteration at 80, loss: 0.02661\n",
      "Iteration at 90, loss: 0.02744\n",
      "epo 98 , acc: 18.443550414885593 % training time: 2.114502429962158\n",
      "Iteration at 10, loss: 0.02707\n",
      "Iteration at 20, loss: 0.02614\n",
      "Iteration at 30, loss: 0.02705\n",
      "Iteration at 40, loss: 0.02588\n",
      "Iteration at 50, loss: 0.02698\n",
      "Iteration at 60, loss: 0.02789\n",
      "Iteration at 70, loss: 0.02619\n",
      "Iteration at 80, loss: 0.02566\n",
      "Iteration at 90, loss: 0.02704\n",
      "epo 99 , acc: 18.141815438772944 % training time: 2.125974416732788\n",
      "Iteration at 10, loss: 0.02655\n",
      "Iteration at 20, loss: 0.02779\n",
      "Iteration at 30, loss: 0.02684\n",
      "Iteration at 40, loss: 0.02624\n",
      "Iteration at 50, loss: 0.02812\n",
      "Iteration at 60, loss: 0.02746\n",
      "Iteration at 70, loss: 0.02669\n",
      "Iteration at 80, loss: 0.02704\n",
      "Iteration at 90, loss: 0.02636\n",
      "epo 100 , acc: 17.94065878803118 % training time: 2.255880117416382\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Apr  6 22:35:27 2021\n",
    "\n",
    "1-1a: testing testing的狀況和training差不多，資料數量比較小訓練上稍稍有快一點。\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train = np.load(r'C:\\Users\\Jimmy\\Desktop\\suzy\\DL homework\\DL_HW1\\Problem1\\MedMNIST\\train.npz')\n",
    "test = np.load(r'C:\\Users\\Jimmy\\Desktop\\suzy\\DL homework\\DL_HW1\\Problem1\\MedMNIST\\test.npz')\n",
    "#list(X_train.keys()) #image,label\n",
    "#np.shape(X_test['image'])\n",
    "X_train = np.array(train['image'])/255\n",
    "Y_train = np.array(train['label'])\n",
    "Y_train = Y_train.reshape(51000,1)\n",
    "yy=Y_train.reshape(51000,)\n",
    "Y_train_=pd.get_dummies(yy)\n",
    "X_train = X_train.reshape(51000, 32*32).astype('float32')\n",
    "\n",
    "\n",
    "\n",
    "X_test = np.array(test['image'])/255\n",
    "Y_test = np.array(test['label'])\n",
    "Y_test = Y_test.reshape(7954,1)\n",
    "yy=Y_test.reshape(7954,)\n",
    "Y_test_=pd.get_dummies(yy)\n",
    "X_test = X_test.reshape(7954, 32*32).astype('float32')\n",
    "x = X_test\n",
    "y = Y_test_\n",
    "\n",
    "# activation function\n",
    "  \n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "# weight initializing => random\n",
    "def initial_weight(a, b):   #layer input & output size\n",
    "    l =[]\n",
    "    for i in range(a * b):\n",
    "        l.append(np.random.randn())\n",
    "    return np.array(l).reshape(a, b)\n",
    "  \n",
    "def f_forward(x, w1, w2):\n",
    "    # hidden\n",
    "    z1 = x.dot(w1)# input from layer 1 \n",
    "    a1 = sigmoid(z1)# out put of layer 2 \n",
    "      \n",
    "    # Output layer\n",
    "    z2 = a1.dot(w2)# input of out layer\n",
    "    a2 = sigmoid(z2)# output of out layer\n",
    "    return a2\n",
    "   \n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    \n",
    "    for i in range(exp_x.shape[0]):\n",
    "        sum_val = exp_x[i].sum()\n",
    "        for j in range(exp_x.shape[1]):\n",
    "            exp_x[i,j] = exp_x[i,j]/sum_val\n",
    "        \n",
    "    return exp_x\n",
    "\n",
    "\n",
    "\n",
    "# cross entropy\n",
    "def loss(y,t): #y is predicted result; t is target\n",
    "    #x=np.log(softmax(x))\n",
    "    y = softmax(y)\n",
    "    \n",
    "    Cross_en_loss = abs(np.sum(-t*np.log(y+1e-6)))/1000\n",
    "    \n",
    "    return Cross_en_loss/1000\n",
    "\n",
    "\n",
    "# Back propagation  \n",
    "def back_prop(x, y, w1, w2, alpha,batch_size):\n",
    "     \n",
    "    w1_adj = 0\n",
    "    w2_adj = 0\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        x_input = x[i]\n",
    "        y_input = y[i]\n",
    "        z1 = x_input.dot(w1) #(10,)\n",
    "        a1 = sigmoid(z1)  #(10,)1\n",
    "          \n",
    "        # Output layer\n",
    "        z2 = a1.dot(w2)#(6,)\n",
    "        a2 = sigmoid(z2)# output of out layer (6,)\n",
    "        \n",
    "        d2 =(a2-y_input) #(6,)\n",
    "        \n",
    "        # Gradient for w1 and w2\n",
    "        #w1_adj = x.reshape(len(x[1]),1).dot(d1.reshape(1,len(d1))) \n",
    "        w2_adj += np.reshape(d2*sigmoid(z2)*(1-sigmoid(z2)),(-1,1)).dot(a1.reshape(1,-1)).T  #(10,6)\n",
    "        \n",
    "        w1_adj += np.reshape((d2*sigmoid(z2)*(1-sigmoid(z2))).T.dot(w2.T)*sigmoid(z1)*(1-sigmoid(z1)),(-1,1)).dot(np.reshape(x_input,(1,-1))).T   #(1024,10)\n",
    "        \n",
    "        \n",
    "    w1_adj = w1_adj/batch_size \n",
    "    w2_adj = w2_adj/batch_size \n",
    "    \n",
    "    # Updating parameters\n",
    "    w1 = w1-(alpha*(w1_adj))\n",
    "    w2 = w2-(alpha*(w2_adj))\n",
    "      \n",
    "    return w1, w2\n",
    "  \n",
    "def train(x, Y, y_true, w1, w2, alpha = 0.10, epoch = 10, batch_size=82):\n",
    "    acc =[]\n",
    "    losss =[]\n",
    "    \n",
    "    for j in range(epoch):\n",
    "        l =[]\n",
    "        pred = []\n",
    "        pred = np.array(pred)\n",
    "        \n",
    "        idx=np.arange(7954)\n",
    "        np.random.shuffle(idx)\n",
    "        x = x[idx,:]\n",
    "        Y = Y[idx,:]\n",
    "        yyy = y_true[idx]\n",
    "        start = time.time()\n",
    "        for i in range(0,97):\n",
    "                       \n",
    "            x_train= x[(i*batch_size):(i+1)*batch_size,:]\n",
    "            target = Y[(i*batch_size):(i+1)*batch_size,:]\n",
    "            Y_train = f_forward(x_train,w1,w2)\n",
    "            yhat = predict(x_train,w1,w2)\n",
    "            \n",
    "            loss_value = loss(Y_train,target)\n",
    "            \n",
    "            pred = np.append(pred,yhat)\n",
    "            l.append(loss_value)\n",
    "            \n",
    "            w1, w2 = back_prop(x_train, target, w1, w2, alpha, batch_size)\n",
    "                \n",
    "            if(i%10==0 and i!=0):\n",
    "                print('Iteration at %d, loss: %.5f' %(i, l[i]))\n",
    "                \n",
    "        final = time.time()\n",
    "        training_time = final-start\n",
    "        print(\"epo\", j + 1, \", acc:\", sum(yyy==pred)/len(yyy)*100,\"% training time:\", training_time)   \n",
    "        #acc.append((1-(sum(l)/len(x)))*100)\n",
    "        acc.append(sum(yyy==pred)/len(yyy)*100)\n",
    "        losss.append(sum(l)/len(x))\n",
    "    return acc, losss, w1, w2\n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def predict(x, w1, w2):\n",
    "    y_pred = f_forward(x, w1, w2)\n",
    "    \n",
    "    maxm = 0\n",
    "    array = []\n",
    "    for i in range(0,y_pred.shape[0]):\n",
    "        target = 0\n",
    "        maxm=y_pred[i,0]\n",
    "        for j in range(0,y_pred.shape[1]):\n",
    "            \n",
    "            if(j==0):\n",
    "                next\n",
    "            \n",
    "            if(maxm<=y_pred[i,j]):\n",
    "                maxm = y_pred[i,j]\n",
    "                target = j\n",
    "            elif(maxm>y_pred[i,j]):\n",
    "                next\n",
    "                \n",
    "        array.append(target)\n",
    "    \n",
    "    return array\n",
    "\n",
    "w1 = initial_weight(1024, 40)\n",
    "w2 = initial_weight(40, 6)\n",
    "\n",
    "y = np.array(y)\n",
    "\n",
    "acc, losss, w1, w2 = train(x, y, yy, w1, w2, 0.05, 100, batch_size=82)\n",
    "y_pred= predict(X_train,w1,w2)\n",
    "#y_pred = f_forward(X_test, w1, w2)\n",
    "plt.plot(np.array(losss))\n",
    "plt.ylabel('LOSS')\n",
    "plt.show()\n",
    "plt.plot(np.array(acc))\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Apr  6 22:35:27 2021\n",
    "\n",
    "1-1c : zero-initialization\n",
    "\n",
    "原本期待W不會被更新，但是看到W有在更新。因為dL/dw都有微幅在增加和訓練，以致W本身開始變動。\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train = np.load(r'C:\\Users\\Jimmy\\Desktop\\suzy\\DL homework\\DL_HW1\\Problem1\\MedMNIST\\train.npz')\n",
    "test = np.load(r'C:\\Users\\Jimmy\\Desktop\\suzy\\DL homework\\DL_HW1\\Problem1\\MedMNIST\\test.npz')\n",
    "\n",
    "X_train = np.array(train['image'])/255\n",
    "Y_train = np.array(train['label'])\n",
    "\n",
    "Y_train = Y_train.reshape(51000,1)\n",
    "\n",
    "yy=Y_train.reshape(51000,)\n",
    "Y_train_=pd.get_dummies(yy)\n",
    "\n",
    "\n",
    "\n",
    "X_train = X_train.reshape(51000, 32*32).astype('float32')\n",
    "\n",
    "\n",
    "x = X_train\n",
    "y = Y_train_\n",
    "\n",
    "\n",
    "# activation function\n",
    "  \n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "# weight 0  initializing => random\n",
    "def initial_weight(a, b):  \n",
    "    l =[]\n",
    " \n",
    "    for i in range(a * b):\n",
    "        l.append(0)\n",
    "    return np.array(l).reshape(a, b)\n",
    "\n",
    "\n",
    "def f_forward(x, w1, w2):\n",
    "    # hidden\n",
    "    z1 = x.dot(w1)# input from layer 1 \n",
    "    a1 = sigmoid(z1)# out put of layer 2 \n",
    "      \n",
    "    # Output layer\n",
    "    z2 = a1.dot(w2)# input of out layer\n",
    "    a2 = sigmoid(z2)# output of out layer\n",
    "    return a2\n",
    "   \n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    \n",
    "    for i in range(exp_x.shape[0]):\n",
    "        sum_val = exp_x[i].sum()\n",
    "        for j in range(exp_x.shape[1]):\n",
    "            exp_x[i,j] = exp_x[i,j]/sum_val\n",
    "        \n",
    "    return exp_x\n",
    "\n",
    "\n",
    "# cross entropy\n",
    "def loss(y,t): #y is predicted result; t is target\n",
    "    #x=np.log(softmax(x))\n",
    "    y = softmax(y)\n",
    "    \n",
    "    Cross_en_loss = abs(np.sum(-t*np.log(y+1e-6)))/1000\n",
    "    \n",
    "    return Cross_en_loss/1000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# Back propagation  \n",
    "def back_prop(x, y, w1, w2, alpha,batch_size):\n",
    "     \n",
    "    w1_adj = 0\n",
    "    w2_adj = 0\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        x_input = x[i]\n",
    "        y_input = y[i]\n",
    "        z1 = x_input.dot(w1) #(10,)\n",
    "        a1 = sigmoid(z1)  #(10,)1\n",
    "          \n",
    "        # Output layer\n",
    "        z2 = a1.dot(w2)#(6,)\n",
    "        a2 = sigmoid(z2)# output of out layer (6,)\n",
    "        \n",
    "        d2 =(a2-y_input) #(6,)\n",
    "        \n",
    "        # Gradient for w1 and w2\n",
    "        #w1_adj = x.reshape(len(x[1]),1).dot(d1.reshape(1,len(d1))) \n",
    "        w2_adj += np.reshape(d2*sigmoid(z2)*(1-sigmoid(z2)),(-1,1)).dot(a1.reshape(1,-1)).T  #(10,6)\n",
    "        \n",
    "        w1_adj += np.reshape((d2*sigmoid(z2)*(1-sigmoid(z2))).T.dot(w2.T)*sigmoid(z1)*(1-sigmoid(z1)),(-1,1)).dot(np.reshape(x_input,(1,-1))).T   #(1024,10)\n",
    "        \n",
    "        \n",
    "    w1_adj = w1_adj/batch_size \n",
    "    w2_adj = w2_adj/batch_size \n",
    "    \n",
    "    # Updating parameters\n",
    "    w1 = w1-(alpha*(w1_adj))\n",
    "    w2 = w2-(alpha*(w2_adj))\n",
    "      \n",
    "    return w1, w2\n",
    "  \n",
    "def train(x, Y, y_true, w1, w2, alpha = 0.10, epoch = 10, batch_size=1000):\n",
    "    acc =[]\n",
    "    losss =[]\n",
    "    \n",
    "    for j in range(epoch):\n",
    "        l =[]\n",
    "        pred = []\n",
    "        pred = np.array(pred)\n",
    "        \n",
    "        idx=np.arange(51000)\n",
    "        np.random.shuffle(idx)\n",
    "        x = x[idx,:]\n",
    "        Y = Y[idx,:]\n",
    "        yyy = y_true[idx]\n",
    "        start = time.time()\n",
    "        for i in range(0,51):\n",
    "                       \n",
    "            x_train= x[(i*batch_size):(i+1)*batch_size,:]\n",
    "            target = Y[(i*batch_size):(i+1)*batch_size,:]\n",
    "            Y_train = f_forward(x_train,w1,w2)\n",
    "            yhat = predict(x_train,w1,w2)\n",
    "            \n",
    "            loss_value = loss(Y_train,target)\n",
    "            \n",
    "            pred = np.append(pred,yhat)\n",
    "            l.append(loss_value)\n",
    "            \n",
    "            w1, w2 = back_prop(x_train, target, w1, w2, alpha, batch_size)\n",
    "                \n",
    "            if(i%10==0 and i!=0):\n",
    "                print('Iteration at %d, loss: %.5f' %(i, l[i]))\n",
    "                \n",
    "        final = time.time()\n",
    "        training_time = final-start\n",
    "        print(\"epo\", j + 1, \", acc:\", sum(yyy==pred)/len(yyy)*100,\"% training time:\", training_time)   \n",
    "        #acc.append((1-(sum(l)/len(x)))*100)\n",
    "        acc.append(sum(yyy==pred)/len(yyy)*100)\n",
    "        losss.append(sum(l)/len(x))\n",
    "    return acc, losss, w1, w2\n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def predict(x, w1, w2):\n",
    "    y_pred = f_forward(x, w1, w2)\n",
    "    \n",
    "    maxm = 0\n",
    "    array = []\n",
    "    for i in range(0,y_pred.shape[0]):\n",
    "        target = 0\n",
    "        maxm=y_pred[i,0]\n",
    "        for j in range(0,y_pred.shape[1]):\n",
    "            \n",
    "            if(j==0):\n",
    "                next\n",
    "            \n",
    "            if(maxm<=y_pred[i,j]):\n",
    "                maxm = y_pred[i,j]\n",
    "                target = j\n",
    "            elif(maxm>y_pred[i,j]):\n",
    "                next\n",
    "                \n",
    "        array.append(target)\n",
    "    \n",
    "    return array\n",
    "\n",
    "w1 = initial_weight(1024, 40)\n",
    "w2 = initial_weight(40, 6)\n",
    "\n",
    "y = np.array(y)\n",
    "\n",
    "acc, losss, w1, w2 = train(x, y, yy, w1, w2, 0.05, 100, batch_size=1000)\n",
    "y_pred= predict(X_train,w1,w2)\n",
    "#y_pred = f_forward(X_test, w1, w2)\n",
    "plt.plot(np.array(losss))\n",
    "plt.ylabel('LOSS')\n",
    "plt.show()\n",
    "plt.plot(np.array(acc))\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Apr  6 22:35:27 2021\n",
    "\n",
    "1-1b :\n",
    "\n",
    "batch size : 1000===>,100,3000,5100\n",
    "batch size = 5100 時觀察到訓練時間增加。\n",
    "             3000 時觀察到訓練時間增加\n",
    "             100  時   \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train = np.load(r'C:\\Users\\Jimmy\\Desktop\\suzy\\DL homework\\DL_HW1\\Problem1\\MedMNIST\\train.npz')\n",
    "test = np.load(r'C:\\Users\\Jimmy\\Desktop\\suzy\\DL homework\\DL_HW1\\Problem1\\MedMNIST\\test.npz')\n",
    "#list(X_train.keys()) #image,label\n",
    "#np.shape(X_test['image'])\n",
    "X_train = np.array(train['image'])/255\n",
    "Y_train = np.array(train['label'])\n",
    "\n",
    "Y_train = Y_train.reshape(51000,1)\n",
    "#Y_train = repr(Y_train) #camma\n",
    "yy=Y_train.reshape(51000,)\n",
    "Y_train_=pd.get_dummies(yy)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "X_train = X_train.reshape(51000, 32*32).astype('float32')\n",
    "\n",
    "\n",
    "x = X_train\n",
    "y = Y_train_\n",
    "\n",
    "\n",
    "# activation function\n",
    "  \n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "# weight initializing => random\n",
    "def initial_weight(a, b):   #layer input & output size\n",
    "    l =[]\n",
    "    for i in range(a * b):\n",
    "        l.append(np.random.randn())\n",
    "    return np.array(l).reshape(a, b)\n",
    "  \n",
    "def f_forward(x, w1, w2):\n",
    "    # hidden\n",
    "    z1 = x.dot(w1)# input from layer 1 \n",
    "    a1 = sigmoid(z1)# out put of layer 2 \n",
    "      \n",
    "    # Output layer\n",
    "    z2 = a1.dot(w2)# input of out layer\n",
    "    a2 = sigmoid(z2)# output of out layer\n",
    "    return a2\n",
    "   \n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    \n",
    "    for i in range(exp_x.shape[0]):\n",
    "        sum_val = exp_x[i].sum()\n",
    "        for j in range(exp_x.shape[1]):\n",
    "            exp_x[i,j] = exp_x[i,j]/sum_val\n",
    "        \n",
    "    return exp_x\n",
    "\n",
    "\n",
    "# cross entropy\n",
    "def loss(y,t): #y is predicted result; t is target\n",
    "    #x=np.log(softmax(x))\n",
    "    y = softmax(y)\n",
    "    \n",
    "    Cross_en_loss = abs(np.sum(-t*np.log(y+1e-6)))/1000\n",
    "    \n",
    "    return Cross_en_loss/1000\n",
    "\n",
    "\n",
    "    \n",
    "# Back propagation  \n",
    "def back_prop(x, y, w1, w2, alpha,batch_size):\n",
    "     \n",
    "    w1_adj = 0\n",
    "    w2_adj = 0\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        x_input = x[i]\n",
    "        y_input = y[i]\n",
    "        z1 = x_input.dot(w1) #(10,)\n",
    "        a1 = sigmoid(z1)  #(10,)1\n",
    "          \n",
    "        # Output layer\n",
    "        z2 = a1.dot(w2)#(6,)\n",
    "        a2 = sigmoid(z2)# output of out layer (6,)\n",
    "        \n",
    "        d2 =(a2-y_input) #(6,)\n",
    "        \n",
    "        # Gradient for w1 and w2\n",
    "        #w1_adj = x.reshape(len(x[1]),1).dot(d1.reshape(1,len(d1))) \n",
    "        w2_adj += np.reshape(d2*sigmoid(z2)*(1-sigmoid(z2)),(-1,1)).dot(a1.reshape(1,-1)).T  #(10,6)\n",
    "        \n",
    "        w1_adj += np.reshape((d2*sigmoid(z2)*(1-sigmoid(z2))).T.dot(w2.T)*sigmoid(z1)*(1-sigmoid(z1)),(-1,1)).dot(np.reshape(x_input,(1,-1))).T   #(1024,10)\n",
    "        \n",
    "        \n",
    "    w1_adj = w1_adj/batch_size \n",
    "    w2_adj = w2_adj/batch_size \n",
    "    \n",
    "    # Updating parameters\n",
    "    w1 = w1-(alpha*(w1_adj))\n",
    "    w2 = w2-(alpha*(w2_adj))\n",
    "      \n",
    "    return w1, w2\n",
    "  \n",
    "def train(x, Y, y_true, w1, w2, alpha = 0.10, epoch = 10, batch_size = 3000 ):\n",
    "    acc =[]\n",
    "    losss =[]\n",
    "    \n",
    "    for j in range(epoch):\n",
    "        l =[]\n",
    "        pred = []\n",
    "        pred = np.array(pred)\n",
    "        \n",
    "        idx=np.arange(51000)\n",
    "        np.random.shuffle(idx)\n",
    "        x = x[idx,:]\n",
    "        Y = Y[idx,:]\n",
    "        yyy = y_true[idx]\n",
    "        start = time.time()\n",
    "        for i in range(0,510):\n",
    "                       \n",
    "            x_train= x[(i*batch_size):(i+1)*batch_size,:]\n",
    "            target = Y[(i*batch_size):(i+1)*batch_size,:]\n",
    "            Y_train = f_forward(x_train,w1,w2)\n",
    "            yhat = predict(x_train,w1,w2)\n",
    "            \n",
    "            loss_value = loss(Y_train,target)\n",
    "            \n",
    "            pred = np.append(pred,yhat)\n",
    "            l.append(loss_value)\n",
    "            \n",
    "            w1, w2 = back_prop(x_train, target, w1, w2, alpha, batch_size)\n",
    "                \n",
    "            if(i%100==0 and i!=0):\n",
    "                print('Iteration at %d, loss: %.5f' %(i, l[i]))\n",
    "                \n",
    "        final = time.time()\n",
    "        training_time = final-start\n",
    "        print(\"epo\", j + 1, \", acc:\", sum(yyy==pred)/len(yyy)*100,\"% training time:\", training_time)   \n",
    "        \n",
    "        acc.append(sum(yyy==pred)/len(yyy)*100)\n",
    "        losss.append(sum(l)/len(x))\n",
    "    return acc, losss, w1, w2\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "def predict(x, w1, w2):\n",
    "    y_pred = f_forward(x, w1, w2)\n",
    "    \n",
    "    maxm = 0\n",
    "    array = []\n",
    "    for i in range(0,y_pred.shape[0]):\n",
    "        target = 0\n",
    "        maxm=y_pred[i,0]\n",
    "        for j in range(0,y_pred.shape[1]):\n",
    "            \n",
    "            if(j==0):\n",
    "                next\n",
    "            \n",
    "            if(maxm<=y_pred[i,j]):\n",
    "                maxm = y_pred[i,j]\n",
    "                target = j\n",
    "            elif(maxm>y_pred[i,j]):\n",
    "                next\n",
    "                \n",
    "        array.append(target)\n",
    "    \n",
    "    return array\n",
    "\n",
    "w1 = initial_weight(1024, 40)\n",
    "w2 = initial_weight(40, 6)\n",
    "\n",
    "y = np.array(y)\n",
    "\n",
    "acc, losss, w1, w2 = train(x, y, yy, w1, w2, 0.05, 100, batch_size)\n",
    "y_pred= predict(X_train,w1,w2)\n",
    "#y_pred = f_forward(X_test, w1, w2)\n",
    "plt.plot(np.array(losss))\n",
    "plt.ylabel('LOSS')\n",
    "plt.show()\n",
    "plt.plot(np.array(acc))\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n"
   ]
  }
 ]
}